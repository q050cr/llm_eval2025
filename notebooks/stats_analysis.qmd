---
title: "LLM Performance in Clinical Question Answering: Results"
author: "Christoph Reich"
date: "`r Sys.Date()`"
format:
  html:
    toc: true
    toc-depth: 3
    code-fold: true
    theme: cosmo
  pdf:
    toc: true
    number-sections: true
    colorlinks: true
bibliography: references.bib
---

```{r setup, include=FALSE}
# Load required libraries
library(tidyverse)
library(knitr)
library(kableExtra)
library(ggplot2)
library(patchwork)
library(lme4)
library(lmerTest)
library(emmeans)
library(irr)
library(psych)
library(corrplot)
library(viridis)
library(ggsignif)
library(ggrepel)
library(factoextra)
library(dendextend)
library(quanteda)
library(cowplot)
library(broom.mixed)
library(performance)
library(see)

# Set global options
knitr::opts_chunk$set(
  echo = TRUE,
  warning = FALSE,
  message = FALSE,
  fig.width = 10,
  fig.height = 8,
  dpi = 300
)

# Set theme for all plots
theme_set(theme_minimal(base_size = 16) +
  theme(
    panel.grid.minor = element_blank(),
    plot.title = element_text(size = 18, face = "bold"),
    axis.title = element_text(size = 16),
    legend.position = "bottom"
  ))

# Define color palettes based on the visualization playbook
brand_colors <- list(
  primary_navy = "#2C4B73",
  accent_yellow = "#F4C430",
  supporting_blue = "#7BA7BC",
  light_gray = "#E8E8E8",
  white = "#FFFFFF",
  alert_red = "#D73027",
  success_green = "#1A9850"
)

# AI company brand colors
ai_colors <- c(
  "openai" = "#17A683",
  "google" = "#528DD5",
  "deepseek" = "#5370FE",
  "perplexity" = "#278491",
  "anthropic" = "#D5A583",
  "xai" = "#000000"
)

# Define rater groups
student_raters <- c("Isabel_LLM", "Jule_LLM", "Lasse_LLM")
expert_raters <- c("Christoph_LLM", "Musti_LLM", "Charlotte_LLM")
auto_raters <- c(
  "autoLLM_gemini_2.5_pro", "autoLLM_gemini_2.0_flash",
  "autoLLM_anthropic_somnet3.5", "autoLLM_anthropic_somnet4",
  "autoLLM_openai_4o_strict", "autoLLM_openai_4o"
)

# Define rating features
rating_features <- c(
  "Appropriateness", "Comprehensibility", "Completeness",
  "Conciseness", "Confabulation Avoidance", "Readability",
  "Educational Value", "Actionability", "Tone/Empathy"
)
```


```{r load-data}
# Load the data
df <- read_csv("./data/data_preparation/individual_ratings_20250606_115527.csv", show_col_types = FALSE)

# Create rater type categories
df <- df %>%
  mutate(
    rater_type_detailed = case_when(
      rater_name %in% student_raters ~ "Student",
      rater_name %in% expert_raters ~ "Expert",
      rater_name %in% auto_raters ~ "Auto-grader",
      TRUE ~ "Unknown"
    ),
    human_vs_auto = ifelse(grader_type == "human", "Human", "Automated")
  )

# Reshape data to long format for easier analysis
df_long <- df %>%
  pivot_longer(
    cols = all_of(rating_features),
    names_to = "feature",
    values_to = "rating"
  ) %>%
  mutate(
    feature = factor(feature, levels = rating_features),
    model = factor(model),
    rater_name = factor(rater_name)
  )
```


# Results

## 1. Descriptive Statistics

### 1.1 Response Characteristics

```{r response-readability}
# Calculate readability metrics for responses
readability_metrics <- df %>%
  group_by(model) %>%
  summarise(
    n_responses = n_distinct(query_id),
    avg_word_count = mean(str_count(response, "\\S+"), na.rm = TRUE),
    sd_word_count = sd(str_count(response, "\\S+"), na.rm = TRUE),
    avg_sentences = mean(str_count(response, "[.!?]+"), na.rm = TRUE),
    sd_sentences = sd(str_count(response, "[.!?]+"), na.rm = TRUE)
  )

# Create table
readability_table <- readability_metrics %>%
  mutate(
    Word_Count = sprintf("%.1f ± %.1f", avg_word_count, sd_word_count),
    Sentences = sprintf("%.1f ± %.1f", avg_sentences, sd_sentences)
  ) %>%
  select(Model = model, `Response Count` = n_responses, `Word Count` = Word_Count, Sentences)

kable(readability_table,
  caption = "Table 1. Response characteristics by model (mean ± SD)",
  format = "latex",
  booktabs = TRUE
) %>%
  kable_styling(latex_options = c("striped", "hold_position"))
```

### 1.2 Overall Performance by Model

```{r overall-model-performance}
# Calculate overall performance metrics by model
model_performance <- df_long %>%
  group_by(model) %>%
  summarise(
    mean_rating = mean(rating, na.rm = TRUE),
    sd_rating = sd(rating, na.rm = TRUE),
    se_rating = sd_rating / sqrt(n()),
    n_ratings = n()
  )

# Create figure
p_model_overall <- ggplot(model_performance, aes(x = reorder(model, mean_rating), y = mean_rating)) +
  geom_bar(stat = "identity", aes(fill = model), width = 0.7) +
  scale_fill_manual(values = ai_colors) +
  geom_errorbar(aes(ymin = mean_rating - se_rating, ymax = mean_rating + se_rating),
    width = 0.3, color = brand_colors$alert_red
  ) +
  geom_text(aes(label = sprintf("%.2f", mean_rating)),
    vjust = -0.5, size = 4, color = brand_colors$primary_navy
  ) +
  coord_cartesian(ylim = c(0, 5)) +
  labs(
    title = "Overall Model Performance Across All Features",
    subtitle = "Mean rating ± SE across all raters and features",
    x = "Model",
    y = "Mean Rating (1-5 scale)"
  ) +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))

print(p_model_overall)

# Save for publication
ggsave("figures/figure1_overall_model_performance.pdf", p_model_overall, width = 8, height = 6)
```

### 1.3 Overall Performance by Feature

```{r overall-feature-performance}
# Calculate overall performance metrics by feature
feature_performance <- df_long %>%
  group_by(feature) %>%
  summarise(
    mean_rating = mean(rating, na.rm = TRUE),
    sd_rating = sd(rating, na.rm = TRUE),
    se_rating = sd_rating / sqrt(n()),
    n_ratings = n()
  )

# Create horizontal bar chart
p_feature_overall <- ggplot(
  feature_performance,
  aes(x = reorder(feature, mean_rating), y = mean_rating)
) +
  geom_bar(stat = "identity", fill = brand_colors$supporting_blue, width = 0.7) +
  geom_errorbar(aes(ymin = mean_rating - se_rating, ymax = mean_rating + se_rating),
    width = 0.3, color = brand_colors$alert_red
  ) +
  geom_text(aes(label = sprintf("%.2f", mean_rating)),
    hjust = -0.2, size = 4, color = brand_colors$primary_navy
  ) +
  coord_flip(xlim = c(0.5, 9.5), ylim = c(0, 5)) +
  labs(
    title = "Overall Feature Ratings Across All Models",
    subtitle = "Mean rating ± SE across all models and raters",
    x = "Feature",
    y = "Mean Rating (1-5 scale)"
  )

print(p_feature_overall)

# Save for publication
ggsave("figures/figure2_overall_feature_performance.pdf", p_feature_overall, width = 8, height = 6)
```

### 1.4 Per-Rater Summary Statistics

```{r rater-summary}
# Calculate per-rater statistics
rater_summary <- df_long %>%
  group_by(rater_name, rater_type_detailed) %>%
  summarise(
    mean_rating = mean(rating, na.rm = TRUE),
    sd_rating = sd(rating, na.rm = TRUE),
    median_rating = median(rating, na.rm = TRUE),
    n_ratings = n(),
    .groups = "drop"
  ) %>%
  arrange(rater_type_detailed, desc(mean_rating)) |>
  # pseudonymize human raters
  group_by(rater_type_detailed) %>% # group temporarily for row_number
  mutate(
    rater_display = case_when(
      rater_type_detailed == "Expert" ~ paste("Expert", row_number()),
      rater_type_detailed == "Student" ~ paste("Student", row_number()),
      rater_type_detailed == "Auto-grader" ~ case_when(
        str_detect(rater_name, "openai_4o_strict") ~ "OpenAI GPT-4o (Stricter)",
        str_detect(rater_name, "openai_4o") ~ "OpenAI GPT-4o",
        str_detect(rater_name, "gemini_2.5_pro") ~ "Gemini 2.5 Pro",
        str_detect(rater_name, "gemini_2.0_flash") ~ "Gemini 2.0 Flash",
        str_detect(rater_name, "anthropic_somnet4") ~ "Anthropic Sonnet 4",
        str_detect(rater_name, "anthropic_somnet3.5") ~ "Anthropic Sonnet 3.5",
        TRUE ~ rater_name
      )
    )
  ) %>%
  ungroup()

# Create visualization
p_rater_summary <- ggplot(
  rater_summary,
  aes(
    x = reorder(rater_display, mean_rating), y = mean_rating,
    fill = rater_type_detailed
  )
) +
  geom_bar(stat = "identity", width = 0.7) +
  geom_errorbar(
    aes(
      ymin = mean_rating - sd_rating / sqrt(n_ratings),
      ymax = mean_rating + sd_rating / sqrt(n_ratings)
    ),
    width = 0.3
  ) +
  geom_hline(
    yintercept = mean(rater_summary$mean_rating),
    linetype = "dashed", color = brand_colors$alert_red
  ) +
  scale_fill_manual(values = c(
    "Student" = brand_colors$supporting_blue,
    "Expert" = brand_colors$primary_navy,
    "Auto-grader" = brand_colors$accent_yellow
  )) +
  coord_flip() +
  labs(
    title = "Individual Rater Tendencies",
    subtitle = "Mean rating ± SE across all features and models",
    x = "Rater",
    y = "Mean Rating (1-5 scale)",
    fill = "Rater Type"
  )

print(p_rater_summary)

# Create summary table
rater_table <- rater_summary %>%
  mutate(
    Rating = sprintf("%.2f ± %.2f", mean_rating, sd_rating)
  ) %>%
  select(
    `Rater` = rater_display, `Type` = rater_type_detailed,
    `Mean ± SD` = Rating, `Median` = median_rating, `N` = n_ratings
  )

kableExtra::kable(rater_table,
  caption = "Table 2. Individual rater summary statistics",
  #format = "latex",
  booktabs = TRUE
) %>%
  kable_styling(latex_options = c("striped", "hold_position"))
```

## 2. Feature-Specific Performance Analysis

### 2.1 Model Performance by Feature

```{r model-feature-heatmap}
# Calculate mean ratings for model-feature combinations
model_feature_matrix <- df_long %>%
  group_by(model, feature) %>%
  summarise(mean_rating = mean(rating, na.rm = TRUE), .groups = "drop") %>%
  pivot_wider(names_from = feature, values_from = mean_rating) %>%
  column_to_rownames("model") %>%
  as.matrix()

# Create heatmap
p_heatmap <- model_feature_matrix %>%
  as.data.frame() %>%
  rownames_to_column("model") %>%
  pivot_longer(-model, names_to = "feature", values_to = "rating") %>%
  mutate(
    model = factor(model, levels = names(ai_colors), labels = tools::toTitleCase(names(ai_colors))),
    feature = factor(feature, levels = rating_features)
  ) %>%
  ggplot(aes(x = feature, y = model, fill = rating)) +
  geom_tile(color = "white", linewidth = 0.5) +
  geom_text(aes(label = sprintf("%.2f", rating)), size = 3.5) +
  scale_fill_gradient2(
    low = brand_colors$alert_red,
    mid = brand_colors$accent_yellow,
    high = brand_colors$success_green,
    midpoint = 3,
    limits = c(1, 5),
    name = "Rating"
  ) +
  labs(
    title = "Model Performance Across Features",
    subtitle = "Mean ratings averaged across all raters",
    x = "Feature",
    y = "Model"
  ) +
  theme(
    axis.text.x = element_text(angle = 45, hjust = 1, size = 10),
    axis.text.y = element_text(size = 10),
    legend.position = "right"
  )

print(p_heatmap)

# Save for publication
ggsave("figures/figure3_model_feature_heatmap.pdf", p_heatmap, width = 10, height = 8)
```

### 2.2 Rater Type Comparison by Feature

```{r rater-type-feature}
# Calculate mean ratings by rater type and feature
rater_type_feature <- df_long %>%
  group_by(rater_type_detailed, feature) %>%
  summarise(
    mean_rating = mean(rating, na.rm = TRUE),
    se_rating = sd(rating, na.rm = TRUE) / sqrt(n()),
    .groups = "drop"
  )

# Create grouped bar chart
p_rater_type_feature <- ggplot(
  rater_type_feature,
  aes(x = feature, y = mean_rating, fill = rater_type_detailed)
) +
  geom_bar(stat = "identity", position = position_dodge(width = 0.8), width = 0.7) +
  geom_errorbar(aes(ymin = mean_rating - se_rating, ymax = mean_rating + se_rating),
    position = position_dodge(width = 0.8), width = 0.3
  ) +
  scale_fill_manual(values = c(
    "Student" = brand_colors$supporting_blue,
    "Expert" = brand_colors$primary_navy,
    "Auto-grader" = brand_colors$accent_yellow
  )) +
  labs(
    title = "Feature Ratings by Rater Type",
    subtitle = "Comparison of rating patterns across different evaluator groups",
    x = "Feature",
    y = "Mean Rating (1-5 scale)",
    fill = "Rater Type"
  ) +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))

ggsave("figures/figure4_mean_ratings_ratergroup_feature.pdf", p_rater_type_feature, width = 10, height = 8)
```

## 3. Inter-Rater Reliability Analysis

### 3.1 Within-Group Reliability

```{r within-group-icc}
calculate_icc <- function(data, rater_group, feature_name) {
  # Create a unique item identifier combining question (index) and model
  icc_data <- data %>%
    filter(rater_name %in% rater_group, feature == feature_name) %>%
    mutate(item_id = paste(index, model, sep = "_")) %>%
    select(item_id, rater_name, rating) %>%
    # Remove any potential duplicates
    distinct(item_id, rater_name, .keep_all = TRUE) %>%
    pivot_wider(names_from = rater_name, values_from = rating) %>%
    column_to_rownames("item_id")

  # Check if we have enough data for ICC calculation
  if (ncol(icc_data) > 1 && nrow(icc_data) > 1) {
    # Remove rows with all NA values
    icc_data_clean <- icc_data[rowSums(!is.na(icc_data)) > 1, ]

    if (nrow(icc_data_clean) > 1 && ncol(icc_data_clean) > 1) {
      tryCatch(
        {
          icc_result <- ICC(icc_data_clean)
          return(data.frame(
            Feature = feature_name,
            N_items = nrow(icc_data_clean),
            N_raters = ncol(icc_data_clean),
            ICC2k = icc_result$results["ICC2k", "ICC"],
            ICC2k_lower = icc_result$results["ICC2k", "lower bound"],
            ICC2k_upper = icc_result$results["ICC2k", "upper bound"],
            p_value = icc_result$results["ICC2k", "p"]
          ))
        },
        error = function(e) {
          return(data.frame(
            Feature = feature_name,
            N_items = nrow(icc_data_clean),
            N_raters = ncol(icc_data_clean),
            ICC2k = NA,
            ICC2k_lower = NA,
            ICC2k_upper = NA,
            p_value = NA,
            Error = as.character(e)
          ))
        }
      )
    }
  }

  return(data.frame(
    Feature = feature_name,
    N_items = ifelse(exists("icc_data"), nrow(icc_data), 0),
    N_raters = ifelse(exists("icc_data"), ncol(icc_data), 0),
    ICC2k = NA,
    ICC2k_lower = NA,
    ICC2k_upper = NA,
    p_value = NA,
    Error = "Insufficient data"
  ))
}
```

```{r}
# Calculate ICC for each group and feature
icc_results <- list()

# Students
icc_results$students <- map_df(
  rating_features,
  ~ calculate_icc(df_long, student_raters, .x)
) %>%
  mutate(Group = "Students")

# Experts
icc_results$experts <- map_df(
  rating_features,
  ~ calculate_icc(df_long, expert_raters, .x)
) %>%
  mutate(Group = "Experts")

# Auto-graders
icc_results$auto <- map_df(
  rating_features,
  ~ calculate_icc(df_long, auto_raters, .x)
) %>%
  mutate(Group = "Auto-graders")

# Combine results
icc_combined <- bind_rows(icc_results)

# Create visualization
p_icc_within <- ggplot(
  icc_combined,
  aes(x = Feature, y = ICC2k, color = Group, group = Group)
) +
  geom_point(size = 3, position = position_dodge(width = 0.3)) +
  geom_errorbar(aes(ymin = ICC2k_lower, ymax = ICC2k_upper),
    width = 0.2, position = position_dodge(width = 0.3)
  ) +
  geom_hline(
    yintercept = 0.6, linetype = "dashed", color = brand_colors$alert_red,
    size = 1, alpha = 0.7
  ) +
  scale_color_manual(values = c(
    "Students" = brand_colors$supporting_blue,
    "Experts" = brand_colors$primary_navy,
    "Auto-graders" = brand_colors$accent_yellow
  )) +
  labs(
    title = "Within-Group Inter-Rater Reliability",
    subtitle = "ICC(2,k) with 95% confidence intervals; dashed line indicates ICC = 0.60 threshold",
    x = "Feature",
    y = "ICC(2,k)",
    color = "Rater Group"
  ) +
  theme(axis.text.x = element_text(angle = 45, hjust = 1)) +
  coord_cartesian(ylim = c(0, 1))

print(p_icc_within)

# Create summary table
icc_table <- icc_combined %>%
  mutate(
    ICC_CI = sprintf("%.3f [%.3f, %.3f]", ICC2k, ICC2k_lower, ICC2k_upper),
    Significance = ifelse(p_value < 0.001, "***",
      ifelse(p_value < 0.01, "**",
        ifelse(p_value < 0.05, "*", "ns")
      )
    )
  ) %>%
  select(Feature, Group, `ICC(2,k) [95% CI]` = ICC_CI, Sig = Significance) %>%
  pivot_wider(names_from = Group, values_from = c(`ICC(2,k) [95% CI]`, Sig)) %>%
  select(Feature, contains("Students"), contains("Experts"), contains("Auto"))

kable(icc_table,
  caption = "Table 3. Within-group inter-rater reliability (ICC(2,k)) by feature",
  format = "latex",
  booktabs = TRUE
) %>%
  kable_styling(latex_options = c("striped", "hold_position", "scale_down"))
```

### 3.2 Between-Group Reliability

```{r between-group-icc}
# Calculate ICC for different group combinations
calculate_between_group_icc <- function(data, group1_raters, group2_raters, feature_name, group_label) {
  all_raters <- c(group1_raters, group2_raters)

  icc_data <- data %>%
    filter(rater_name %in% all_raters, feature == feature_name) %>%
    select(query_id, rater_name, rating) %>%
    pivot_wider(names_from = rater_name, values_from = rating) %>%
    select(-query_id) %>%
    as.matrix()

  if (ncol(icc_data) > 1 && nrow(icc_data) > 1) {
    icc_result <- ICC(icc_data)
    return(data.frame(
      Feature = feature_name,
      Comparison = group_label,
      ICC2k = icc_result$results["ICC2k", "ICC"],
      ICC2k_lower = icc_result$results["ICC2k", "lower bound"],
      ICC2k_upper = icc_result$results["ICC2k", "upper bound"],
      p_value = icc_result$results["ICC2k", "p"]
    ))
  } else {
    return(data.frame(
      Feature = feature_name,
      Comparison = group_label,
      ICC2k = NA,
      ICC2k_lower = NA,
      ICC2k_upper = NA,
      p_value = NA
    ))
  }
}

# Calculate between-group ICCs
between_group_results <- list()

# Students vs Experts
between_group_results$stud_exp <- map_df(
  rating_features,
  ~ calculate_between_group_icc(df_long, student_raters, expert_raters, .x, "Students vs Experts")
)

# Human (all) vs Auto
human_raters <- c(student_raters, expert_raters)
between_group_results$human_auto <- map_df(
  rating_features,
  ~ calculate_between_group_icc(df_long, human_raters, auto_raters, .x, "Human vs Auto")
)

# Experts vs Auto
between_group_results$exp_auto <- map_df(
  rating_features,
  ~ calculate_between_group_icc(df_long, expert_raters, auto_raters, .x, "Experts vs Auto")
)

# All raters combined
all_raters <- c(student_raters, expert_raters, auto_raters)
between_group_results$all <- map_df(
  rating_features,
  ~ calculate_between_group_icc(df_long, all_raters, character(0), .x, "All Raters")
)

# Combine results
between_icc_combined <- bind_rows(between_group_results)

# Create visualization
p_icc_between <- ggplot(
  between_icc_combined %>%
    filter(Comparison != "All Raters"),
  aes(x = Feature, y = ICC2k, color = Comparison, group = Comparison)
) +
  geom_point(size = 3, position = position_dodge(width = 0.3)) +
  geom_errorbar(aes(ymin = ICC2k_lower, ymax = ICC2k_upper),
    width = 0.2, position = position_dodge(width = 0.3)
  ) +
  geom_hline(
    yintercept = 0.6, linetype = "dashed", color = brand_colors$alert_red,
    size = 1, alpha = 0.7
  ) +
  scale_color_manual(values = c(
    "Students vs Experts" = brand_colors$primary_navy,
    "Human vs Auto" = brand_colors$supporting_blue,
    "Experts vs Auto" = brand_colors$accent_yellow
  )) +
  labs(
    title = "Between-Group Inter-Rater Reliability",
    subtitle = "ICC(2,k) for different rater group combinations",
    x = "Feature",
    y = "ICC(2,k)",
    color = "Group Comparison"
  ) +
  theme(axis.text.x = element_text(angle = 45, hjust = 1)) +
  coord_cartesian(ylim = c(0, 1))

print(p_icc_between)
```

## 4. Model Performance Comparison

### 4.1 Linear Mixed-Effects Models

```{r lmm-analysis}
# Function to fit LMM for each feature
fit_feature_lmm <- function(data, feature_name) {
  feature_data <- data %>%
    filter(feature == feature_name)

  # Fit the model with random intercepts for question and rater
  model <- lmer(rating ~ model + (1 | query_id) + (1 | rater_name),
    data = feature_data, REML = FALSE
  )

  # Get model summary
  model_summary <- summary(model)

  # Perform likelihood ratio test
  null_model <- lmer(rating ~ 1 + (1 | query_id) + (1 | rater_name),
    data = feature_data, REML = FALSE
  )
  lr_test <- anova(null_model, model)

  # Get estimated marginal means
  emm <- emmeans(model, ~model)
  emm_df <- as.data.frame(emm)

  # Perform pairwise comparisons
  pairs <- pairs(emm, adjust = "tukey")

  return(list(
    feature = feature_name,
    model = model,
    summary = model_summary,
    lr_test = lr_test,
    emmeans = emm_df,
    pairwise = pairs
  ))
}

# Fit models for all features
lmm_results <- map(rating_features, ~ fit_feature_lmm(df_long, .x))
names(lmm_results) <- rating_features

# Extract key statistics for each feature
feature_model_stats <- map_df(lmm_results, function(res) {
  lr_p <- res$lr_test$`Pr(>Chisq)`[2]
  data.frame(
    Feature = res$feature,
    LR_Chisq = res$lr_test$Chisq[2],
    df = res$lr_test$Df[2],
    p_value = lr_p,
    Significant = ifelse(lr_p < 0.001, "***",
      ifelse(lr_p < 0.01, "**",
        ifelse(lr_p < 0.05, "*", "ns")
      )
    )
  )
})

# Create estimated marginal means plot for significant features
significant_features <- feature_model_stats %>%
  filter(p_value < 0.05) %>%
  pull(Feature)

if (length(significant_features) > 0) {
  emm_data <- map_df(significant_features, function(feat) {
    lmm_results[[feat]]$emmeans %>%
      mutate(Feature = feat)
  })

  p_emm <- ggplot(emm_data, aes(x = model, y = emmean, color = Feature)) +
    geom_point(position = position_dodge(width = 0.5), size = 3) +
    geom_errorbar(aes(ymin = lower.CL, ymax = upper.CL),
      position = position_dodge(width = 0.5), width = 0.3
    ) +
    facet_wrap(~Feature, scales = "free_x", ncol = 3) +
    labs(
      title = "Estimated Marginal Means by Model",
      subtitle = "Features with significant model differences (p < 0.05)",
      x = "Model",
      y = "Estimated Mean Rating",
      color = "Feature"
    ) +
    theme(
      axis.text.x = element_text(angle = 45, hjust = 1),
      legend.position = "none"
    )

  print(p_emm)
}

# Create summary table
kable(feature_model_stats,
  caption = "Table 4. Likelihood ratio tests for model effects by feature",
  format = "latex",
  booktabs = TRUE,
  digits = c(0, 2, 0, 4, 0)
) %>%
  kable_styling(latex_options = c("striped", "hold_position"))
```

Pr(>Chisq)`[2]
  ))
}

# Fit models for all features
rater_type_results <- map(rating_features, ~fit_rater_type_lmm(df_long, .x))
names(rater_type_results) <- rating_features

# Extract rater type main effects
rater_type_stats <- map_df(rater_type_results, function(res) {
  # Get estimated marginal means for rater types
  emm <- emmeans(res$model_no_int, ~ rater_type_detailed)
  emm_df <- as.data.frame(emm)
  
  # Perform pairwise comparisons
  pairs <- pairs(emm, adjust = "tukey")
  pairs_df <- as.data.frame(pairs)
  
  # Create summary
  data.frame(
    Feature = res$feature,
    Student_Mean = emm_df$emmean[emm_df$rater_type_detailed == "Student"],
    Expert_Mean = emm_df$emmean[emm_df$rater_type_detailed == "Expert"],
    Auto_Mean = emm_df$emmean[emm_df$rater_type_detailed == "Auto-grader"],
    Interaction_p = res$interaction_p,
    Student_Expert_p = pairs_df$p.value[pairs_df$contrast == "Student - Expert"],
    Student_Auto_p = pairs_df$p.value[pairs_df$contrast == "Student - Auto-grader"],
    Expert_Auto_p = pairs_df$p.value[pairs_df$contrast == "Expert - Auto-grader"]
  )
})

# Create visualization for rater type differences
p_rater_differences <- rater_type_stats %>%
  pivot_longer(cols = c(Student_Mean, Expert_Mean, Auto_Mean),
               names_to = "Rater_Type",
               values_to = "Mean_Rating") %>%
  mutate(Rater_Type = str_remove(Rater_Type, "_Mean")) %>%
  ggplot(aes(x = Feature, y = Mean_Rating, color = Rater_Type, group = Rater_Type)) +
  geom_point(size = 3, position = position_dodge(width = 0.3)) +
  geom_line(position = position_dodge(width = 0.3), alpha = 0.5) +
  scale_color_manual(values = c("Student" = brand_colors$supporting_blue,
                               "Expert" = brand_colors$primary_navy,
                               "Auto" = brand_colors$accent_yellow)) +
  labs(
    title = "Rater Type Effects Across Features",
    subtitle = "Estimated marginal means from mixed-effects models",
    x = "Feature",
    y = "Mean Rating",
    color = "Rater Type"
  ) +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))

print(p_rater_differences)

# Create summary table
rater_type_table <- rater_type_stats %>%
  mutate(
    Student = sprintf("%.2f", Student_Mean),
    Expert = sprintf("%.2f", Expert_Mean),
    `Auto-grader` = sprintf("%.2f", Auto_Mean),
    `S-E` = ifelse(Student_Expert_p < 0.001, "***",
                   ifelse(Student_Expert_p < 0.01, "**",
                         ifelse(Student_Expert_p < 0.05, "*", "ns"))),
    `S-A` = ifelse(Student_Auto_p < 0.001, "***",
                   ifelse(Student_Auto_p < 0.01, "**",
                         ifelse(Student_Auto_p < 0.05, "*", "ns"))),
    `E-A` = ifelse(Expert_Auto_p < 0.001, "***",
                   ifelse(Expert_Auto_p < 0.01, "**",
                         ifelse(Expert_Auto_p < 0.05, "*", "ns")))
  ) %>%
  select(Feature, Student, Expert, `Auto-grader`, `S-E`, `S-A`, `E-A`)

kable(rater_type_table, 
      caption = "Table 5. Rater type effects and pairwise comparisons",
      format = "latex", 
      booktabs = TRUE) %>%
  kable_styling(latex_options = c("striped", "hold_position")) %>%
  add_header_above(c(" " = 4, "Pairwise Comparisons" = 3))
```

## 5. Multivariate Analysis

### 5.1 Principal Component Analysis

```{r pca-analysis}
# Prepare data for PCA - average ratings by model and feature
pca_data <- df_long %>%
  group_by(model, feature) %>%
  summarise(mean_rating = mean(rating, na.rm = TRUE), .groups = "drop") %>%
  pivot_wider(names_from = feature, values_from = mean_rating) %>%
  column_to_rownames("model")

# Perform PCA
pca_result <- prcomp(pca_data, scale = TRUE, center = TRUE)

# Calculate variance explained
var_explained <- summary(pca_result)$importance[2,] * 100
cumvar_explained <- summary(pca_result)$importance[3,] * 100

# Create scree plot
scree_data <- data.frame(
  PC = 1:length(var_explained),
  Variance = var_explained,
  Cumulative = cumvar_explained
)

p_scree <- ggplot(scree_data, aes(x = PC)) +
  geom_bar(aes(y = Variance), stat = "identity", fill = brand_colors$primary_navy) +
  geom_line(aes(y = Cumulative), color = brand_colors$alert_red, size = 1.5) +
  geom_point(aes(y = Cumulative), color = brand_colors$alert_red, size = 3) +
  geom_hline(yintercept = 80, linetype = "dashed", color = brand_colors$accent_yellow) +
  scale_x_continuous(breaks = 1:9) +
  labs(
    title = "PCA Scree Plot",
    subtitle = "Variance explained by principal components",
    x = "Principal Component",
    y = "Variance Explained (%)"
  )

print(p_scree)

# Create biplot
fviz_pca_biplot(pca_result, 
                col.var = brand_colors$primary_navy,
                col.ind = brand_colors$accent_yellow,
                label = "all",
                labelsize = 4,
                pointsize = 3,
                repel = TRUE) +
  labs(title = "PCA Biplot: Models and Features",
       subtitle = "First two principal components") +
  theme_minimal()

# Feature loadings
loadings_df <- as.data.frame(pca_result$rotation[, 1:2])
loadings_df$feature <- rownames(loadings_df)

p_loadings <- ggplot(loadings_df, aes(x = PC1, y = PC2, label = feature)) +
  geom_segment(aes(x = 0, y = 0, xend = PC1, yend = PC2), 
               arrow = arrow(length = unit(0.3, "cm")),
               color = brand_colors$supporting_blue) +
  geom_text_repel(size = 4, color = brand_colors$primary_navy) +
  geom_hline(yintercept = 0, linetype = "dashed", alpha = 0.5) +
  geom_vline(xintercept = 0, linetype = "dashed", alpha = 0.5) +
  labs(
    title = "Feature Loadings on First Two Principal Components",
    x = sprintf("PC1 (%.1f%% variance)", var_explained[1]),
    y = sprintf("PC2 (%.1f%% variance)", var_explained[2])
  ) +
  coord_equal()

print(p_loadings)

# Create loadings table
loadings_table <- loadings_df %>%
  arrange(desc(abs(PC1))) %>%
  mutate(
    PC1 = sprintf("%.3f", PC1),
    PC2 = sprintf("%.3f", PC2)
  ) %>%
  select(Feature = feature, PC1, PC2)

kable(loadings_table, 
      caption = "Table 6. Feature loadings on first two principal components",
      format = "latex", 
      booktabs = TRUE) %>%
  kable_styling(latex_options = c("striped", "hold_position"))
```

### 5.2 Cluster Analysis of Models

```{r cluster-analysis}
# Perform hierarchical clustering
dist_matrix <- dist(pca_data, method = "euclidean")
hclust_result <- hclust(dist_matrix, method = "ward.D2")

# Convert to dendrogram
dend <- as.dendrogram(hclust_result)
dend <- color_branches(dend, k = 3, col = c(
  brand_colors$primary_navy,
  brand_colors$accent_yellow,
  brand_colors$supporting_blue
))

# Create dendrogram plot
p_dendrogram <- ggplot(as.ggdend(dend)) +
  labs(
    title = "Hierarchical Clustering of Models",
    subtitle = "Based on average ratings across all features",
    x = "Model",
    y = "Distance"
  ) +
  theme(axis.text.x = element_text(angle = 0, hjust = 0.5))

print(p_dendrogram)

# K-means clustering
set.seed(123)
kmeans_result <- kmeans(pca_data, centers = 3, nstart = 25)

# Add cluster assignments to PCA plot
pca_scores <- as.data.frame(pca_result$x[, 1:2])
pca_scores$model <- rownames(pca_scores)
pca_scores$cluster <- factor(kmeans_result$cluster)

p_clusters <- ggplot(pca_scores, aes(x = PC1, y = PC2, color = cluster, label = model)) +
  geom_point(size = 4) +
  geom_text_repel(size = 4) +
  scale_color_manual(values = c(
    brand_colors$primary_navy,
    brand_colors$accent_yellow,
    brand_colors$supporting_blue
  )) +
  labs(
    title = "Model Clusters in PCA Space",
    subtitle = "K-means clustering (k=3)",
    x = sprintf("PC1 (%.1f%% variance)", var_explained[1]),
    y = sprintf("PC2 (%.1f%% variance)", var_explained[2]),
    color = "Cluster"
  )

print(p_clusters)
```

### 5.3 Auto-grader vs Human Consensus Analysis

```{r auto-human-comparison}
# Calculate human consensus scores
human_consensus <- df_long %>%
  filter(grader_type == "human") %>%
  group_by(query_id, model, feature) %>%
  summarise(human_mean = mean(rating, na.rm = TRUE), .groups = "drop")

# Calculate individual auto-grader correlations with human consensus
auto_correlations <- df_long %>%
  filter(grader_type == "auto") %>%
  left_join(human_consensus, by = c("query_id", "model", "feature")) %>%
  group_by(rater_name, feature) %>%
  summarise(
    correlation = cor(rating, human_mean, use = "complete.obs"),
    n_pairs = sum(!is.na(rating) & !is.na(human_mean)),
    .groups = "drop"
  )

# Create heatmap of correlations
correlation_matrix <- auto_correlations %>%
  pivot_wider(names_from = feature, values_from = correlation) %>%
  column_to_rownames("rater_name") %>%
  as.matrix()

# Create heatmap
p_auto_human_corr <- correlation_matrix %>%
  as.data.frame() %>%
  rownames_to_column("Auto_Grader") %>%
  pivot_longer(-Auto_Grader, names_to = "Feature", values_to = "Correlation") %>%
  mutate(
    Auto_Grader = str_remove(Auto_Grader, "autoLLM_"),
    Feature = factor(Feature, levels = rating_features)
  ) %>%
  ggplot(aes(x = Feature, y = Auto_Grader, fill = Correlation)) +
  geom_tile(color = "white", size = 0.5) +
  geom_text(aes(label = sprintf("%.2f", Correlation)), size = 3) +
  scale_fill_gradient2(
    low = brand_colors$alert_red,
    mid = "white",
    high = brand_colors$success_green,
    midpoint = 0.7,
    limits = c(0.4, 1),
    name = "Correlation"
  ) +
  labs(
    title = "Auto-grader Agreement with Human Consensus",
    subtitle = "Pearson correlation coefficients",
    x = "Feature",
    y = "Auto-grader"
  ) +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))

print(p_auto_human_corr)

# Calculate RMSE and bias for each auto-grader
auto_performance <- df_long %>%
  filter(grader_type == "auto") %>%
  left_join(human_consensus, by = c("query_id", "model", "feature")) %>%
  group_by(rater_name) %>%
  summarise(
    RMSE = sqrt(mean((rating - human_mean)^2, na.rm = TRUE)),
    MAE = mean(abs(rating - human_mean), na.rm = TRUE),
    Bias = mean(rating - human_mean, na.rm = TRUE),
    Correlation = cor(rating, human_mean, use = "complete.obs"),
    .groups = "drop"
  ) %>%
  mutate(rater_name = str_remove(rater_name, "autoLLM_"))

# Create performance comparison plot
p_auto_performance <- auto_performance %>%
  pivot_longer(
    cols = c(RMSE, MAE, Bias, Correlation),
    names_to = "Metric",
    values_to = "Value"
  ) %>%
  ggplot(aes(x = rater_name, y = Value, fill = Metric)) +
  geom_bar(stat = "identity", position = "dodge") +
  facet_wrap(~Metric, scales = "free_y", ncol = 2) +
  scale_fill_manual(values = c(
    "RMSE" = brand_colors$alert_red,
    "MAE" = brand_colors$accent_yellow,
    "Bias" = brand_colors$supporting_blue,
    "Correlation" = brand_colors$success_green
  )) +
  labs(
    title = "Auto-grader Performance Metrics",
    subtitle = "Comparison with human consensus ratings",
    x = "Auto-grader",
    y = "Value"
  ) +
  theme(
    axis.text.x = element_text(angle = 45, hjust = 1),
    legend.position = "none"
  )

print(p_auto_performance)

# Create summary table
auto_performance_table <- auto_performance %>%
  mutate(
    RMSE = sprintf("%.3f", RMSE),
    MAE = sprintf("%.3f", MAE),
    Bias = sprintf("%.3f", Bias),
    Correlation = sprintf("%.3f", Correlation)
  ) %>%
  rename(`Auto-grader` = rater_name)

kable(auto_performance_table,
  caption = "Table 7. Auto-grader performance metrics compared to human consensus",
  format = "latex",
  booktabs = TRUE
) %>%
  kable_styling(latex_options = c("striped", "hold_position"))
```

## 6. Summary and Key Findings

### 6.1 Overall Performance Summary

```{r summary-visualization}
# Create comprehensive summary plot
# Combine key findings into a single publication-ready figure

# Panel A: Model rankings
panel_a <- df_long %>%
  group_by(model) %>%
  summarise(
    mean_rating = mean(rating, na.rm = TRUE),
    se = sd(rating, na.rm = TRUE) / sqrt(n()),
    .groups = "drop"
  ) %>%
  ggplot(aes(x = reorder(model, mean_rating), y = mean_rating)) +
  geom_bar(stat = "identity", fill = brand_colors$primary_navy) +
  geom_errorbar(aes(ymin = mean_rating - se, ymax = mean_rating + se),
    width = 0.3
  ) +
  coord_flip() +
  labs(
    title = "A. Overall Model Performance",
    x = "Model", y = "Mean Rating"
  ) +
  theme_minimal(base_size = 10)

# Panel B: Feature reliability
panel_b <- icc_combined %>%
  filter(Group == "All Raters") %>%
  ggplot(aes(x = reorder(Feature, ICC2k), y = ICC2k)) +
  geom_bar(stat = "identity", fill = brand_colors$supporting_blue) +
  geom_hline(yintercept = 0.6, linetype = "dashed", color = brand_colors$alert_red) +
  coord_flip() +
  labs(
    title = "B. Feature Reliability (All Raters)",
    x = "Feature", y = "ICC(2,k)"
  ) +
  theme_minimal(base_size = 10)

# Panel C: Rater type agreement
panel_c <- df_long %>%
  group_by(rater_type_detailed) %>%
  summarise(
    mean_rating = mean(rating, na.rm = TRUE),
    sd = sd(rating, na.rm = TRUE),
    .groups = "drop"
  ) %>%
  ggplot(aes(x = rater_type_detailed, y = mean_rating, fill = rater_type_detailed)) +
  geom_bar(stat = "identity") +
  geom_errorbar(aes(ymin = mean_rating - sd, ymax = mean_rating + sd),
    width = 0.3
  ) +
  scale_fill_manual(values = c(
    "Student" = brand_colors$supporting_blue,
    "Expert" = brand_colors$primary_navy,
    "Auto-grader" = brand_colors$accent_yellow
  )) +
  labs(
    title = "C. Rater Type Comparison",
    x = "Rater Type", y = "Mean Rating"
  ) +
  theme_minimal(base_size = 10) +
  theme(legend.position = "none")

# Panel D: Auto-grader accuracy
panel_d <- auto_performance %>%
  ggplot(aes(x = reorder(rater_name, -Correlation), y = Correlation)) +
  geom_bar(stat = "identity", fill = brand_colors$success_green) +
  geom_hline(yintercept = 0.8, linetype = "dashed", color = brand_colors$alert_red) +
  labs(
    title = "D. Auto-grader Correlation with Humans",
    x = "Auto-grader", y = "Correlation"
  ) +
  theme_minimal(base_size = 10) +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))

# Combine panels
summary_plot <- (panel_a | panel_b) / (panel_c | panel_d) +
  plot_annotation(
    title = "Figure 8. Summary of Key Findings",
    subtitle = "Comprehensive overview of model performance, feature reliability, and rater agreement"
  )

print(summary_plot)

# Save for publication
ggsave("figures/figure8_summary_findings.pdf", summary_plot, width = 12, height = 10)
```

### 6.2 Key Statistical Findings

```{r key-findings-table}
# Create summary table of key findings
key_findings <- data.frame(
  Metric = c(
    "Best Performing Model",
    "Most Reliable Feature",
    "Least Reliable Feature",
    "Human-Auto Agreement",
    "Expert-Student Agreement",
    "Features with Significant Model Differences"
  ),
  Finding = c(
    model_performance %>% arrange(desc(mean_rating)) %>% slice(1) %>% pull(model),
    feature_performance %>% arrange(desc(mean_rating)) %>% slice(1) %>% pull(feature),
    feature_performance %>% arrange(mean_rating) %>% slice(1) %>% pull(feature),
    sprintf("r = %.3f", mean(auto_performance$Correlation)),
    sprintf("ICC = %.3f", mean(between_icc_combined %>%
      filter(Comparison == "Students vs Experts") %>%
      pull(ICC2k), na.rm = TRUE)),
    sum(feature_model_stats$p_value < 0.05)
  ),
  Details = c(
    sprintf(
      "Mean rating = %.2f",
      model_performance %>% arrange(desc(mean_rating)) %>% slice(1) %>% pull(mean_rating)
    ),
    sprintf(
      "Mean rating = %.2f",
      feature_performance %>% arrange(desc(mean_rating)) %>% slice(1) %>% pull(mean_rating)
    ),
    sprintf(
      "Mean rating = %.2f",
      feature_performance %>% arrange(mean_rating) %>% slice(1) %>% pull(mean_rating)
    ),
    "Average across all auto-graders",
    "Average across all features",
    paste(significant_features, collapse = ", ")
  )
)

kable(key_findings,
  caption = "Table 8. Summary of key statistical findings",
  format = "latex",
  booktabs = TRUE
) %>%
  kable_styling(latex_options = c("striped", "hold_position")) %>%
  column_spec(1, width = "5cm") %>%
  column_spec(3, width = "6cm")
```

## 7. Supplementary Materials

### 7.1 Model Diagnostics

```{r model-diagnostics}
# Check assumptions for a representative LMM
representative_model <- lmm_results[["Appropriateness"]]$model

# Create diagnostic plots
p_diagnostics <- plot(check_model(representative_model),
  colors = c(brand_colors$primary_navy, brand_colors$alert_red)
)

# QQ plot of random effects
ranef_df <- as.data.frame(ranef(representative_model))

p_qq_random <- ggplot(ranef_df$query_id, aes(sample = `(Intercept)`)) +
  stat_qq() +
  stat_qq_line(color = brand_colors$alert_red) +
  labs(
    title = "QQ Plot of Random Effects (Query ID)",
    x = "Theoretical Quantiles",
    y = "Sample Quantiles"
  ) +
  theme_minimal()

print(p_qq_random)
```

### 7.2 Complete Pairwise Comparisons

```{r pairwise-full}
# Generate complete pairwise comparison tables for significant features
if (length(significant_features) > 0) {
  pairwise_tables <- map(significant_features, function(feat) {
    pairs_df <- as.data.frame(lmm_results[[feat]]$pairwise)
    pairs_df %>%
      mutate(
        Feature = feat,
        Estimate = sprintf("%.3f", estimate),
        SE = sprintf("%.3f", SE),
        `95% CI` = sprintf("[%.3f, %.3f]", lower.CL, upper.CL),
        p_adj = sprintf("%.4f", p.value),
        Sig = ifelse(p.value < 0.001, "***",
          ifelse(p.value < 0.01, "**",
            ifelse(p.value < 0.05, "*", "")
          )
        )
      ) %>%
      select(Feature, Contrast = contrast, Estimate, SE, `95% CI`, `p-value` = p_adj, Sig)
  })

  # Combine all pairwise comparisons
  all_pairwise <- bind_rows(pairwise_tables)

  # Create supplementary table
  kable(all_pairwise %>% filter(Feature == significant_features[1]),
    caption = sprintf("Table S1. Pairwise model comparisons for %s", significant_features[1]),
    format = "latex",
    booktabs = TRUE
  ) %>%
    kable_styling(latex_options = c("striped", "hold_position", "scale_down"))
}
```

## References

```{r save-session-info}
# Save session information for reproducibility
sink("session_info.txt")
sessionInfo()
sink()

# Print completion message
cat("\nAnalysis completed successfully. All figures saved to 'figures/' directory.\n")
```