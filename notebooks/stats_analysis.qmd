---
title: "LLM Performance in Clinical Question Answering: Results"
author: "Christoph Reich"
date: "`r Sys.Date()`"
format:
  html:
    toc: true
    toc-depth: 3
    code-fold: true
    theme: cosmo
  pdf:
    toc: true
    number-sections: true
    colorlinks: true
bibliography: references.bib
---

```{r setup, include=FALSE}
# Load required libraries
library(tidyverse)
library(knitr)
library(kableExtra)
library(gt)
library(ggplot2)
library(patchwork)
library(quanteda)
library(quanteda.textstats)
library(lme4)
library(lmerTest)
library(emmeans)
library(irr)
library(psych)
library(corrplot)
library(viridis)
library(ggsignif)
library(ggrepel)
library(factoextra)
library(dendextend)
library(quanteda)
library(cowplot)
library(broom.mixed)
library(performance)
library(see)

# Set global options
knitr::opts_chunk$set(
  echo = TRUE,
  warning = FALSE,
  message = FALSE,
  fig.width = 10,
  fig.height = 8,
  dpi = 300
)

# Set theme for all plots
theme_set(theme_minimal(base_size = 16) +
  theme(
    panel.grid.minor = element_blank(),
    plot.title = element_text(size = 18, face = "bold"),
    axis.title = element_text(size = 16),
    legend.position = "bottom"
  ))

# Define color palettes based on the visualization playbook
brand_colors <- list(
  primary_navy = "#2C4B73",
  accent_yellow = "#F4C430",
  supporting_blue = "#7BA7BC",
  light_gray = "#E8E8E8",
  white = "#FFFFFF",
  alert_red = "#D73027",
  success_green = "#1A9850"
)

# AI company brand colors
ai_colors <- c(
  "openai" = "#17A683",
  "google" = "#528DD5",
  "deepseek" = "#5370FE",
  "perplexity" = "#278491",
  "anthropic" = "#D5A583",
  "xai" = "#000000"
)

# Define rater groups
student_raters <- c("Isabel_LLM", "Jule_LLM", "Lasse_LLM")
expert_raters <- c("Christoph_LLM", "Musti_LLM", "Charlotte_LLM")
auto_raters <- c(
  "autoLLM_gemini_2.5_pro", "autoLLM_gemini_2.0_flash",
  "autoLLM_anthropic_somnet3.5", "autoLLM_anthropic_somnet4",
  "autoLLM_openai_4o_strict", "autoLLM_openai_4o"
)

# Define rating features
rating_features <- c(
  "Appropriateness", "Comprehensibility", "Completeness",
  "Conciseness", "Confabulation Avoidance", "Readability",
  "Educational Value", "Actionability", "Tone/Empathy"
)
```


```{r load-data}
# Load the data
df <- read_csv("./data/data_preparation/individual_ratings_20250606_115527.csv", show_col_types = FALSE)

# Create rater type categories
df <- df %>%
  mutate(
    rater_type_detailed = case_when(
      rater_name %in% student_raters ~ "Student",
      rater_name %in% expert_raters ~ "Expert",
      rater_name %in% auto_raters ~ "Auto-grader",
      TRUE ~ "Unknown"
    ),
    human_vs_auto = ifelse(grader_type == "human", "Human", "Automated")
  )

# Reshape data to long format for easier analysis
df_long <- df %>%
  pivot_longer(
    cols = all_of(rating_features),
    names_to = "feature",
    values_to = "rating"
  ) %>%
  mutate(
    feature = factor(feature, levels = rating_features),
    model = factor(model),
    rater_name = factor(rater_name)
  )
```


# Results

## 1. Descriptive Statistics

### 1.1 Response Characteristics

```{r response-readability}

# Prepare unique responses: collapse across raters to one per query/model
unique_responses <- df %>%
  distinct(query_id, model, category, response) %>%
  # Set category factor order
  mutate(
    category = factor(category, levels = c(
      "Disease Understanding and Diagnosis",
      "Treatment and Management",
      "Lifestyle & Daily Activity"
    ))
  )

# Calculate readability metrics for responses
readability_metrics <- unique_responses %>%
  group_by(model) %>%
  summarise(
    n_responses = n_distinct(query_id),
    avg_word_count = mean(str_count(response, "\\S+"), na.rm = TRUE),
    sd_word_count = sd(str_count(response, "\\S+"), na.rm = TRUE),
    avg_sentences = mean(str_count(response, "[.!?]+"), na.rm = TRUE),
    sd_sentences = sd(str_count(response, "[.!?]+"), na.rm = TRUE)
  )

# Create table
readability_table <- readability_metrics %>%
  mutate(
    Word_Count = sprintf("%.1f ± %.1f", avg_word_count, sd_word_count),
    Sentences = sprintf("%.1f ± %.1f", avg_sentences, sd_sentences)
  ) %>%
  select(Model = model, `Response Count` = n_responses, `Word Count` = Word_Count, Sentences)

kable(readability_table,
  caption = "Table 1. Response characteristics by model (mean ± SD)",
  # format = "latex",
  booktabs = TRUE
) %>%
  kable_styling(latex_options = c("striped", "hold_position"))
```


We calculate and present readability metrics for medical text responses using the **quanteda** and **quanteda.textstats** packages. We will compute the following indices:
  
  * **Flesch Reading Ease**
  * **Flesch-Kincaid Grade Level**
  * **Gunning Fog Index**
  * **SMOG Index**
  * **Coleman-Liau Index**
  * **Automated Readability Index (ARI)**
  
  We will present descriptive statistics and visualizations in two ways:
  
  1. **Per model**
  2. **Per model and category**
  

#### Flesch-Based Readability Formulas

1. **Flesch Reading Ease (FRE)**

   * **Formula**: `206.835 – (1.015 × ASL) – (84.6 × ASW)`

     * *ASL*: Average Sentence Length (words per sentence)
     * *ASW*: Average Syllables per Word
   * **Output**: Score ranging from 0 to 100; higher scores indicate easier readability.
   * **Interpretation**:

     * 90–100: Very easy (understood by 11-year-olds)
     * 60–70: Standard (13–15-year-olds)
     * 30–50: Difficult (college level)
   * **Usage**: Provides a general sense of text readability but requires a conversion table to relate scores to grade levels. ([en.wikipedia.org][1], [readable.com][2])

2. **Flesch-Kincaid Grade Level (FKGL)**

   * **Formula**: `0.39 × ASL + 11.8 × ASW – 15.59`
   * **Output**: U.S. school grade level; for example, a score of 8.5 corresponds to an 8th-grade reading level.
   * **Usage**: Directly indicates the education level required to comprehend the text, making it practical for assessing materials intended for specific audiences. ([de.wikipedia.org][3], [nira.com][4])

3. **Flesch.PSK (Powers-Sumner-Kearl Variation)**

   * **Formula**: `(0.0778 × ASL) + (4.55 × ASW) – 2.2029`
   * **Usage**: A less commonly used variation; not widely adopted in current readability assessments.

---

In medical and health-related fields, ensuring that patient education materials are accessible is crucial. Both FRE and FKGL are commonly used to assess the readability of such materials. However, FKGL is often preferred because it provides a direct correlation to U.S. grade levels, simplifying the assessment of whether materials are appropriate for the target audience. ([journals.lww.com][https://journals.lww.com/edhe/fulltext/2017/30010/assessing_reading_levels_of_health_information_.15.aspx])

For instance, a study analyzing patient information sheets found that the mean FKGL was 11.4, indicating that the materials were written at a level suitable for individuals with at least an 11th-grade education. ([pubmed.ncbi.nlm.nih.gov][https://pubmed.ncbi.nlm.nih.gov/36131293/])


* **Use Flesch-Kincaid Grade Level (FKGL)** when you need a straightforward indication of the education level required to understand the text. This is particularly useful in medical contexts, where materials should be tailored to the patient's reading ability.([en.wikipedia.org][7])
* **Use Flesch Reading Ease (FRE)** if you prefer a score that reflects the overall readability on a 100-point scale. This can be helpful for comparing the readability of different texts or versions of the same text.


```{r}
#| label: fn-readability-metrics
#| echo: false

clean_text_for_readability <- function(text) {
  text %>%
    # Remove markdown formatting
    str_replace_all("\\*\\*(.*?)\\*\\*", "\\1") %>%  # Remove **bold**
    str_replace_all("###\\s*", "") %>%               # Remove ### headers
    str_replace_all("#\\s*", "") %>%                 # Remove # headers
    # Clean up special characters that might confuse readability
    str_replace_all("≤", "less than or equal to") %>%
    # Remove extra whitespace
    str_replace_all("\\s+", " ") %>%
    str_trim()
}

# Modified function
compute_readability <- function(texts) {
  cleaned_texts <- map_chr(texts, clean_text_for_readability)
  txt_quanteda <- quanteda::corpus(cleaned_texts)
  stats <- quanteda.textstats::textstat_readability(
    txt_quanteda,
    measure = c("Flesch", "Flesch.Kincaid", "FOG", "SMOG", "Coleman.Liau.short", "ARI")
  )
  as_tibble(stats)
}

# Original function (no cleaning)
compute_readability_raw <- function(texts) {
  txt_quanteda <- quanteda::corpus(texts)  # No cleaning step
  stats <- quanteda.textstats::textstat_readability(
    txt_quanteda,
    measure = c("Flesch", "Flesch.Kincaid", "FOG", "SMOG", "Coleman.Liau.short", "ARI")
  )
  as_tibble(stats)
}
```


```{r}
#| label: compare-cleaned-text-stats-with-raw
#| echo: false
#| include: false

# Test on a few more samples to see if the pattern holds
test_samples <- head(unique_responses$response, 5)  # Test first 5 responses

# Compare raw vs cleaned across multiple texts
comparison_results <- map_dfr(1:length(test_samples), ~{
  text <- test_samples[.x]
  
  raw <- compute_readability_raw(text) %>% mutate(version = "raw", sample = .x)
  cleaned <- compute_readability(clean_text_for_readability(text)) %>% mutate(version = "cleaned", sample = .x)
  
  bind_rows(raw, cleaned)
})

# Check if any differences exist
res_cleaning <- comparison_results %>%
  select(-document) %>%
  pivot_wider(names_from = version, values_from = c(Flesch:ARI)) %>%
  mutate(
    flesch_diff = abs(Flesch_raw - Flesch_cleaned),
    fk_diff = abs(Flesch.Kincaid_raw - Flesch.Kincaid_cleaned)
  ) 

# no diffs observed
```

```{r}
#| label: readability-for-each-response-row-wise

readability_scores <- unique_responses %>%
  mutate(
    rid = row_number(),
    .metrics = compute_readability(response)
  ) %>%
  unnest(cols = c(.metrics))
```


```{r}
#| label: Readability-per-model

read_by_model <- readability_scores %>%
  group_by(model) %>%
  summarise(
    # Count total responses
    n_responses = n(),
    `Flesch Ease`    = sprintf("%.1f ± %.1f", mean(Flesch, na.rm = TRUE), sd(Flesch, na.rm = TRUE)),
    `Flesch K-G`     = sprintf("%.1f ± %.1f", mean(`Flesch.Kincaid`, na.rm = TRUE), sd(`Flesch.Kincaid`, na.rm = TRUE)),
    `Gunning Fog`    = sprintf("%.1f ± %.1f", mean(FOG, na.rm = TRUE), sd(FOG, na.rm = TRUE)),
    SMOG             = sprintf("%.1f ± %.1f", mean(SMOG, na.rm = TRUE), sd(SMOG, na.rm = TRUE)),
    `Coleman-Liau`   = sprintf("%.1f ± %.1f", mean(`Coleman.Liau.short`, na.rm = TRUE), sd(`Coleman.Liau.short`, na.rm = TRUE)),
    ARI              = sprintf("%.1f ± %.1f", mean(ARI, na.rm = TRUE), sd(ARI, na.rm = TRUE))
  )

read_by_model %>% 
  mutate(
    model = tools::toTitleCase(model)
  ) %>% 
  gt() %>%
  tab_header(title = "Table 1: Readability Metrics by Model") %>%
  fmt_missing(missing_text = "-")
```

```{r}
#| label: density-plot-flesch-by-model

ggplot(readability_scores, aes(x = Flesch.Kincaid, fill = model)) +
  geom_density(alpha = 0.4) +
  labs(
    title = "Distribution of Flesch Reading Ease by Model",
    x = "Flesch Reading Ease",
    y = "Density"
  ) +
  scale_fill_manual(values = ai_colors)
```

```{r}
#| label: violin-plot-flesch-model
#| message: false
#| warning: false

p <- ggplot(readability_scores, aes(x = model, y = Flesch.Kincaid, fill = model)) +
  geom_violin(trim = FALSE, alpha = 0.6) +
  geom_jitter(width = 0.2, size = 1.5, alpha = 0.7) +
  stat_summary(fun = mean, geom = "point", shape = 23, size = 3, fill = "white") +
  labs(
    title    = "Flesch Reading Ease by Model",
    subtitle = "Violin + individual scores + mean",
    x        = NULL,
    y        = "Flesch Reading Ease"
  ) +
  scale_fill_manual(values = ai_colors) +
  theme_minimal(base_size = 14) +
  theme(
    # axis.text.x      = element_text(angle = 45, hjust = 1),
    # panel.grid.minor = element_blank(),
    legend.position  = "none"
  )

p
```


```{r}
#| label: Readability-per-category-and-model

read_by_category_model <- readability_scores %>%
  group_by(category, model) %>%
  summarise(
    n_responses = n(),
    `Flesch Ease`    = sprintf("%.1f ± %.1f", mean(Flesch, na.rm = TRUE), sd(Flesch, na.rm = TRUE)),
    `Flesch K-G`     = sprintf("%.1f ± %.1f", mean(Flesch.Kincaid, na.rm = TRUE), sd(Flesch.Kincaid, na.rm = TRUE)),
    `Gunning Fog`    = sprintf("%.1f ± %.1f", mean(FOG, na.rm = TRUE), sd(FOG, na.rm = TRUE)),
    SMOG             = sprintf("%.1f ± %.1f", mean(SMOG, na.rm = TRUE), sd(SMOG, na.rm = TRUE)),
    `Coleman-Liau`   = sprintf("%.1f ± %.1f", mean(`Coleman.Liau.short`, na.rm = TRUE), sd(`Coleman.Liau.short`, na.rm = TRUE)),
    ARI              = sprintf("%.1f ± %.1f", mean(ARI, na.rm = TRUE), sd(ARI, na.rm = TRUE))
  )

read_by_category_model %>% 
  mutate(
    model = tools::toTitleCase(model)
  ) %>% 
  gt() %>%
  tab_header(title = "Table 2: Readability Metrics by Category and Model") %>%
  fmt_missing(missing_text = "-")
```



```{r}
#| label: density-plot-flesch-by-model-category

ggplot(readability_scores, aes(x = Flesch, fill = model)) +
  geom_density(alpha = 0.5) +
  facet_wrap(~ category) +
  labs(
    title = "Flesch Reading Ease across Models by Category",
    x = "Flesch Reading Ease",
    y = "Density"
  ) +
  scale_fill_manual(values = ai_colors)

```

```{r}
#| label: violin-plot-flesch-model-category
#| message: false
#| warning: false

p <- ggplot(readability_scores, aes(x = model, y = Flesch.Kincaid, fill = model)) +
  geom_violin(trim = FALSE, alpha = 0.6) +
  geom_jitter(width = 0.2, size = 1.5, alpha = 0.7) +
  stat_summary(fun = mean, geom = "point", shape = 23, size = 3, fill = "white") +
  facet_wrap(~ category, ncol = 1) +
  labs(
    title    = "Flesch Reading Ease by Model and Category",
    subtitle = "Violin + individual scores + mean",
    x        = NULL,
    y        = "Flesch Reading Ease"
  ) +
  scale_fill_manual(values = ai_colors) +
  theme_minimal(base_size = 14) +
  theme(
    # axis.text.x      = element_text(angle = 45, hjust = 1),
    # panel.grid.minor = element_blank(),
    legend.position  = "none"
  )

p
```



### 1.2 Overall Performance by Model

```{r overall-model-performance}
# Calculate overall performance metrics by model
model_performance <- df_long %>%
  group_by(model) %>%
  summarise(
    mean_rating = mean(rating, na.rm = TRUE),
    sd_rating = sd(rating, na.rm = TRUE),
    se_rating = sd_rating / sqrt(n()),
    n_ratings = n()
  )

# Create figure
p_model_overall <- ggplot(model_performance, aes(x = reorder(model, mean_rating), y = mean_rating)) +
  geom_bar(stat = "identity", aes(fill = model), width = 0.7) +
  scale_fill_manual(values = ai_colors) +
  geom_errorbar(aes(ymin = mean_rating - se_rating, ymax = mean_rating + se_rating),
    width = 0.3, color = brand_colors$alert_red
  ) +
  geom_text(aes(label = sprintf("%.2f", mean_rating)),
    vjust = -0.5, size = 4, color = brand_colors$primary_navy
  ) +
  coord_cartesian(ylim = c(0, 5)) +
  labs(
    title = "Overall Model Performance Across All Features",
    subtitle = "Mean rating ± SE across all raters and features",
    x = "Model",
    y = "Mean Rating (1-5 scale)"
  ) +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))

print(p_model_overall)

# Save for publication
ggsave("figures/figure1_overall_model_performance.pdf", p_model_overall, width = 8, height = 6)
```

### 1.3 Overall Performance by Feature

```{r overall-feature-performance}
# Calculate overall performance metrics by feature
feature_performance <- df_long %>%
  group_by(feature) %>%
  summarise(
    mean_rating = mean(rating, na.rm = TRUE),
    sd_rating = sd(rating, na.rm = TRUE),
    se_rating = sd_rating / sqrt(n()),
    n_ratings = n()
  )

# Create horizontal bar chart
p_feature_overall <- ggplot(
  feature_performance,
  aes(x = reorder(feature, mean_rating), y = mean_rating)
) +
  geom_bar(stat = "identity", fill = brand_colors$supporting_blue, width = 0.7) +
  geom_errorbar(aes(ymin = mean_rating - se_rating, ymax = mean_rating + se_rating),
    width = 0.3, color = brand_colors$alert_red
  ) +
  geom_text(aes(label = sprintf("%.2f", mean_rating)),
    hjust = -0.2, size = 4, color = brand_colors$primary_navy
  ) +
  coord_flip(xlim = c(0.5, 9.5), ylim = c(0, 5)) +
  labs(
    title = "Overall Feature Ratings Across All Models",
    subtitle = "Mean rating ± SE across all models and raters",
    x = "Feature",
    y = "Mean Rating (1-5 scale)"
  )

print(p_feature_overall)

# Save for publication
ggsave("figures/figure2_overall_feature_performance.pdf", p_feature_overall, width = 8, height = 6)
```

### 1.4 Per-Rater Summary Statistics

```{r rater-summary}
# Calculate per-rater statistics
rater_summary <- df_long %>%
  group_by(rater_name, rater_type_detailed) %>%
  summarise(
    mean_rating = mean(rating, na.rm = TRUE),
    sd_rating = sd(rating, na.rm = TRUE),
    median_rating = median(rating, na.rm = TRUE),
    n_ratings = n(),
    .groups = "drop"
  ) %>%
  arrange(rater_type_detailed, desc(mean_rating)) |>
  # pseudonymize human raters
  group_by(rater_type_detailed) %>% # group temporarily for row_number
  mutate(
    rater_display = case_when(
      rater_type_detailed == "Expert" ~ paste("Expert", row_number()),
      rater_type_detailed == "Student" ~ paste("Student", row_number()),
      rater_type_detailed == "Auto-grader" ~ case_when(
        str_detect(rater_name, "openai_4o_strict") ~ "OpenAI GPT-4o (Stricter)",
        str_detect(rater_name, "openai_4o") ~ "OpenAI GPT-4o",
        str_detect(rater_name, "gemini_2.5_pro") ~ "Gemini 2.5 Pro",
        str_detect(rater_name, "gemini_2.0_flash") ~ "Gemini 2.0 Flash",
        str_detect(rater_name, "anthropic_somnet4") ~ "Anthropic Sonnet 4",
        str_detect(rater_name, "anthropic_somnet3.5") ~ "Anthropic Sonnet 3.5",
        TRUE ~ rater_name
      )
    )
  ) %>%
  ungroup()

# Create visualization
p_rater_summary <- ggplot(
  rater_summary,
  aes(
    x = reorder(rater_display, mean_rating), y = mean_rating,
    fill = rater_type_detailed
  )
) +
  geom_bar(stat = "identity", width = 0.7) +
  geom_errorbar(
    aes(
      ymin = mean_rating - sd_rating / sqrt(n_ratings),
      ymax = mean_rating + sd_rating / sqrt(n_ratings)
    ),
    width = 0.3
  ) +
  geom_hline(
    yintercept = mean(rater_summary$mean_rating),
    linetype = "dashed", color = brand_colors$alert_red
  ) +
  scale_fill_manual(values = c(
    "Student" = brand_colors$supporting_blue,
    "Expert" = brand_colors$primary_navy,
    "Auto-grader" = brand_colors$accent_yellow
  )) +
  coord_flip() +
  labs(
    title = "Individual Rater Tendencies",
    subtitle = "Mean rating ± SE across all features and models",
    x = "Rater",
    y = "Mean Rating (1-5 scale)",
    fill = "Rater Type"
  )

print(p_rater_summary)

# Create summary table
rater_table <- rater_summary %>%
  mutate(
    Rating = sprintf("%.2f ± %.2f", mean_rating, sd_rating)
  ) %>%
  select(
    `Rater` = rater_display, `Type` = rater_type_detailed,
    `Mean ± SD` = Rating, `Median` = median_rating, `N` = n_ratings
  )

kableExtra::kable(rater_table,
  caption = "Table 2. Individual rater summary statistics",
  #format = "latex",
  booktabs = TRUE
) %>%
  kable_styling(latex_options = c("striped", "hold_position"))
```

## 2. Feature-Specific Performance Analysis

### 2.1 Model Performance by Feature

```{r model-feature-heatmap}
# Calculate mean ratings for model-feature combinations
model_feature_matrix <- df_long %>%
  group_by(model, feature) %>%
  summarise(mean_rating = mean(rating, na.rm = TRUE), .groups = "drop") %>%
  pivot_wider(names_from = feature, values_from = mean_rating) %>%
  column_to_rownames("model") %>%
  as.matrix()

# Create heatmap
p_heatmap <- model_feature_matrix %>%
  as.data.frame() %>%
  rownames_to_column("model") %>%
  pivot_longer(-model, names_to = "feature", values_to = "rating") %>%
  mutate(
    model = factor(model, levels = names(ai_colors), labels = tools::toTitleCase(names(ai_colors))),
    feature = factor(feature, levels = rating_features)
  ) %>%
  ggplot(aes(x = feature, y = model, fill = rating)) +
  geom_tile(color = "white", linewidth = 0.5) +
  geom_text(aes(label = sprintf("%.2f", rating)), size = 3.5) +
  scale_fill_gradient2(
    low = brand_colors$alert_red,
    mid = brand_colors$accent_yellow,
    high = brand_colors$success_green,
    midpoint = 3,
    limits = c(1, 5),
    name = "Rating"
  ) +
  labs(
    title = "Model Performance Across Features",
    subtitle = "Mean ratings averaged across all raters",
    x = "Feature",
    y = "Model"
  ) +
  theme(
    axis.text.x = element_text(angle = 45, hjust = 1, size = 10),
    axis.text.y = element_text(size = 10),
    legend.position = "right"
  )

print(p_heatmap)

# Save for publication
ggsave("figures/figure3_model_feature_heatmap.pdf", p_heatmap, width = 10, height = 8)
```

```{r}
#| label: feature-stats
#| echo: FALSE
library(dplyr)
library(tidyr)

# 1. Numeric summaries
feature_stats <- df_long %>%
  group_by(model, feature) %>%
  summarise(
    mean_rating = mean(rating),
    sd_rating   = sd(rating),
    n           = n(),
    se          = sd_rating / sqrt(n),
    ci_lower    = mean_rating - qt(0.975, n - 1) * se,
    ci_upper    = mean_rating + qt(0.975, n - 1) * se,
    .groups     = "drop"
  )

# 2. Pivot for display: combine mean & CI
feature_wide <- feature_stats %>%
  mutate(
    Mean_CI = sprintf("%0.2f (%0.2f–%0.2f)", mean_rating, ci_lower, ci_upper)
  ) %>%
  select(feature, model, Mean_CI) %>%
  pivot_wider(names_from  = model,
              values_from = Mean_CI)

# 3. Identify highest and lowest feature for each model
best_worst <- feature_stats %>%
  group_by(model) %>%
  summarise(
    Highest     = feature[which.max(mean_rating)],
    Highest_val = max(mean_rating),
    Lowest      = feature[which.min(mean_rating)],
    Lowest_val  = min(mean_rating),
    .groups     = "drop"
  )
```

```{r}
#| label: feature-table-wide
#| echo: FALSE

feature_wide %>%
  gt(rowname_col = "feature") %>%
  cols_label(
    feature    = "Feature",
    anthropic  = "Anthropic",
    deepseek   = "DeepSeek",
    google     = "Google",
    openai     = "OpenAI",
    perplexity = "Perplexity",
    xai        = "XAI"
  ) %>%
  tab_header(
    title = "Mean Feature Ratings (95% CI) by Model"
  )
```

```{r}
#| label: best-worst-ci
#| echo: FALSE

# calculated feature_stats and best_worst before:
# feature_stats has columns: model, feature, mean_rating, ci_lower, ci_upper
# best_worst has: model, Highest, Highest_val, Lowest, Lowest_val

best_worst_ci <- best_worst %>%
  left_join(
    feature_stats %>% select(model, feature, ci_lower, ci_upper),
    by = c("model", "Highest" = "feature")
  ) %>%
  rename(high_ci_lower = ci_lower,
         high_ci_upper = ci_upper) %>%
  left_join(
    feature_stats %>% select(model, feature, ci_lower, ci_upper),
    by = c("model", "Lowest" = "feature")
  ) %>%
  rename(low_ci_lower = ci_lower,
         low_ci_upper = ci_upper) %>%
  mutate(
    `Top Feature (Mean [95% CI])`    = sprintf("%s (%.2f [%.2f–%.2f])",
                                              Highest, Highest_val, high_ci_lower, high_ci_upper),
    `Bottom Feature (Mean [95% CI])` = sprintf("%s (%.2f [%.2f–%.2f])",
                                              Lowest, Lowest_val, low_ci_lower, low_ci_upper),
    # Capitalize model names
    Model = recode(model,
                   anthropic  = "Anthropic",
                   deepseek   = "DeepSeek",
                   google     = "Google",
                   openai     = "OpenAI",
                   perplexity = "Perplexity",
                   xai        = "XAI")
  ) %>%
  select(Model, `Top Feature (Mean [95% CI])`, `Bottom Feature (Mean [95% CI])`)

# Render GT table
best_worst_ci %>%
  gt() %>%
  cols_label(
    Model                           = "Model",
    `Top Feature (Mean [95% CI])`   = "Highest",
    `Bottom Feature (Mean [95% CI])`= "Lowest"
  ) %>%
  tab_header(
    title = "Best‐ and Worst‐Rated Features by Model"
  )

```


#### Interpretation

* **Anthropic**
  
  * Highest: **`r best_worst$Highest[best_worst$model=="anthropic"]`**
  (mean = `r sprintf("%.2f", best_worst$Highest_val[best_worst$model=="anthropic"])`).
* Lowest: **`r best_worst$Lowest[best_worst$model=="anthropic"]`**
  (mean = `r sprintf("%.2f", best_worst$Lowest_val[best_worst$model=="anthropic"])`).

* **DeepSeek**
  
  * Highest: **`r best_worst$Highest[best_worst$model=="deepseek"]`**
  (mean = `r sprintf("%.2f", best_worst$Highest_val[best_worst$model=="deepseek"])`).
* Lowest: **`r best_worst$Lowest[best_worst$model=="deepseek"]`**
  (mean = `r sprintf("%.2f", best_worst$Lowest_val[best_worst$model=="deepseek"])`).

* **Google**
  
  * Highest: **`r best_worst$Highest[best_worst$model=="google"]`**
  (mean = `r sprintf("%.2f", best_worst$Highest_val[best_worst$model=="google"])`).
* Lowest: **`r best_worst$Lowest[best_worst$model=="google"]`**
  (mean = `r sprintf("%.2f", best_worst$Lowest_val[best_worst$model=="google"])`).

* **OpenAI**
  
  * Highest: **`r best_worst$Highest[best_worst$model=="openai"]`**
  (mean = `r sprintf("%.2f", best_worst$Highest_val[best_worst$model=="openai"])`).
* Lowest: **`r best_worst$Lowest[best_worst$model=="openai"]`**
  (mean = `r sprintf("%.2f", best_worst$Lowest_val[best_worst$model=="openai"])`).

* **Perplexity**
  
  * Highest: **`r best_worst$Highest[best_worst$model=="perplexity"]`**
  (mean = `r sprintf("%.2f", best_worst$Highest_val[best_worst$model=="perplexity"])`).
* Lowest: **`r best_worst$Lowest[best_worst$model=="perplexity"]`**
  (mean = `r sprintf("%.2f", best_worst$Lowest_val[best_worst$model=="perplexity"])`).

* **XAI**
  
  * Highest: **`r best_worst$Highest[best_worst$model=="xai"]`**
  (mean = `r sprintf("%.2f", best_worst$Highest_val[best_worst$model=="xai"])`).
* Lowest: **`r best_worst$Lowest[best_worst$model=="xai"]`**
  (mean = `r sprintf("%.2f", best_worst$Lowest_val[best_worst$model=="xai"])`).

> Clinicians can use these highlights—each model’s strongest and weakest feature—to decide which system best fits their specific priorities.




### 2.2 Rater Type Comparison by Feature

```{r rater-type-feature}
# Calculate mean ratings by rater type and feature
rater_type_feature <- df_long %>%
  group_by(rater_type_detailed, feature) %>%
  summarise(
    mean_rating = mean(rating, na.rm = TRUE),
    se_rating = sd(rating, na.rm = TRUE) / sqrt(n()),
    .groups = "drop"
  )

# Create grouped bar chart
p_rater_type_feature <- ggplot(
  rater_type_feature,
  aes(x = feature, y = mean_rating, fill = rater_type_detailed)
) +
  geom_bar(stat = "identity", position = position_dodge(width = 0.8), width = 0.7) +
  geom_errorbar(aes(ymin = mean_rating - se_rating, ymax = mean_rating + se_rating),
    position = position_dodge(width = 0.8), width = 0.3
  ) +
  scale_fill_manual(values = c(
    "Student" = brand_colors$supporting_blue,
    "Expert" = brand_colors$primary_navy,
    "Auto-grader" = brand_colors$accent_yellow
  )) +
  labs(
    title = "Feature Ratings by Rater Type",
    subtitle = "Comparison of rating patterns across different evaluator groups",
    x = "Feature",
    y = "Mean Rating (1-5 scale)",
    fill = "Rater Type"
  ) +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))

p_rater_type_feature

ggsave("figures/figure4_mean_ratings_ratergroup_feature.pdf", p_rater_type_feature, width = 10, height = 8)
```

## 3. Inter-Rater Reliability Analysis

### 3.1 Within-Group Reliability

We compute the single‐measure, absolute‐agreement ICC (ICC(A,1)) for each rater group (Students, Experts, Auto-graders) using a two-way random‐effects model. First we show ICC for all rating features combined: 


```{r within-group-icc}
#-- Function to compute ICC for a given rater group ------------------------
calculate_group_icc <- function(df_long, group_name) {
  df_long %>%
    filter(rater_type_detailed == group_name) %>% 
    # make a unique "subject" ID so that each row in the wide matrix is one 
    # (question × model × feature) combination
    mutate(subject = paste(index, model, feature, sep = "_")) %>% 
    select(subject, rater_name, rating) %>% 
    # pivot to wide: one row per subject, one column per rater
    pivot_wider(names_from  = rater_name,
                values_from = rating) -> wide_df

  # drop the subject column and coerce to a numeric matrix
  rating_mat <- wide_df %>% select(-subject) %>% as.matrix()

  # compute ICC: two‐way random, absolute agreement, single‐rater
  irr::icc(rating_mat,
           model = "twoway",
           type  = "agreement",
           unit  = "single")
}

#-- Compute for each group --------------------------------------------------
icc_students     <- calculate_group_icc(df_long, "Student")
icc_experts      <- calculate_group_icc(df_long, "Expert")
icc_autograders  <- calculate_group_icc(df_long, "Auto-grader")

#-- View the results --------------------------------------------------------
print(icc_students)
print(icc_experts)
print(icc_autograders)
```

```{r}
#| label: icc-within-group-table-overall-features
#| echo: FALSE

# build a summary tibble with combined CI column
icc_summary <- tibble(
  `Rater Group` = c("Students", "Experts", "Auto-graders"),
  ICC            = c(icc_students$value,
                     icc_experts$value,
                     icc_autograders$value),
  LowerCI        = c(icc_students$lbound,
                     icc_experts$lbound,
                     icc_autograders$lbound),
  UpperCI        = c(icc_students$ubound,
                     icc_experts$ubound,
                     icc_autograders$ubound)
) %>%
  mutate(
    `95% CI` = sprintf("(%0.3f–%0.3f)", LowerCI, UpperCI),
    Interpretation = case_when(
      ICC < 0.50               ~ "Poor agreement",
      ICC < 0.75               ~ "Moderate agreement",
      ICC < 0.90               ~ "Good agreement",
      TRUE                     ~ "Excellent agreement"
    )
  ) %>%
  select(`Rater Group`, ICC, `95% CI`, Interpretation)

# render as a gt table
icc_summary %>%
  gt() %>%
  fmt_number(
    columns = "ICC",
    decimals = 3
  ) %>%
  cols_label(
    ICC           = "ICC(A,1)",
    `95% CI`      = "(95% CI)",
    Interpretation = "Interpretation"
  ) %>%
  tab_header(
    title = "Within‐Group ICC(A,1) Results"
  )
```

#### Interpretation

* **Students** (ICC = `r sprintf("%.3f", icc_summary$`ICC(A,1)`[1])`, 95% CI \[`r sprintf("%.3f", icc_summary$`95% CI Lower`[1])`–`r sprintf("%.3f", icc_summary$`95% CI Upper`[1])`])
Indicates *poor agreement* among student raters.

* **Experts** (ICC = `r sprintf("%.3f", icc_summary$`ICC(A,1)`[2])`, 95% CI \[`r sprintf("%.3f", icc_summary$`95% CI Lower`[2])`–`r sprintf("%.3f", icc_summary$`95% CI Upper`[2])`])
Indicates *poor agreement* among expert raters.

* **Auto-graders** (ICC = `r sprintf("%.3f", icc_summary$`ICC(A,1)`[3])`, 95% CI \[`r sprintf("%.3f", icc_summary$`95% CI Lower`[3])`–`r sprintf("%.3f", icc_summary$`95% CI Upper`[3])`])
Indicates *moderate agreement*, outperforming humans but still below “good.”

All three groups fall below the conventional **0.75** threshold for “good” reliability; auto-graders show the highest consistency, followed by students, with experts showing the greatest variability.

### 3.1. Within-Group Reliability (Cont'd for each feature category)

```{r}
# Function to compute ICC for each feature category
icc_by_feature <- function(df, group_name, feat) {
  mat <- df %>%
    filter(rater_type_detailed == group_name, feature == feat) %>%
    mutate(subject = paste(index, model, sep = "_")) %>%
    select(subject, rater_name, rating) %>%
    pivot_wider(names_from  = rater_name, values_from = rating) %>%
    select(-subject) %>%
    as.matrix()
  
  out <- irr::icc(mat,
                  model = "twoway",
                  type  = "agreement",
                  unit  = "single")
  
  tibble(
    Group    = group_name,
    Feature  = feat,
    ICC      = out$value,
    LowerCI  = out$lbound,
    UpperCI  = out$ubound
  )
}

# Define groups and features
groups   <- c("Student", "Expert", "Auto-grader")
features <- unique(df_long$feature)

# Compute ICC for each combination
icc_feature_all <- map_dfr(groups, function(g) {
  map_dfr(features, function(f) icc_by_feature(df_long, g, f))
})

```


```{r}
#| label: icc-feature-table
#| echo: FALSE

icc_feature_all %>%
  arrange(Group, Feature) %>%
  # create a combined CI column and drop the separate bounds
  mutate(
    `95% CI` = sprintf("(%0.3f–%0.3f)", LowerCI, UpperCI)
  ) %>%
  select(Group, Feature, ICC, `95% CI`) %>%
  gt(
    rowname_col   = "Feature",
    groupname_col = "Group"
  ) %>%
  fmt_number(
    columns = "ICC",
    decimals = 3
  ) %>%
  cols_label(
    ICC   = "ICC(A,1)",
    `95% CI` = "(95% CI)"
  ) %>%
  tab_header(
    title = "Feature‐Wise ICC(A,1) by Rater Group"
  )

```


#### Interpretation

* **Students**
  
  * **Overall** ICC = `r sprintf("%.3f", icc_overall$icc[icc_overall$group=="Student"])`.
* **Highest**: `r icc_feature_all %>% filter(Group=="Student") %>% slice_max(ICC, n = 1) %>% pull(Feature)`
(ICC = `r sprintf("%.3f", icc_feature_all %>% filter(Group=="Student") %>% slice_max(ICC, n = 1) %>% pull(ICC))`).
* **Lowest**: `r icc_feature_all %>% filter(Group=="Student") %>% slice_min(ICC, n = 1) %>% pull(Feature)`
(ICC = `r sprintf("%.3f", icc_feature_all %>% filter(Group=="Student") %>% slice_min(ICC, n = 1) %>% pull(ICC))`).
Students agree best on **Educational Value**, and struggle most with **Confabulation Avoidance**.

* **Experts**
  
  * **Overall** ICC = `r sprintf("%.3f", icc_overall$icc[icc_overall$group=="Expert"])`.
* **Highest**: `r icc_feature_all %>% filter(Group=="Expert") %>% slice_max(ICC, n = 1) %>% pull(Feature)`
(ICC = `r sprintf("%.3f", icc_feature_all %>% filter(Group=="Expert") %>% slice_max(ICC, n = 1) %>% pull(ICC))`).
* **Lowest**: `r icc_feature_all %>% filter(Group=="Expert") %>% slice_min(ICC, n = 1) %>% pull(Feature)`
(ICC = `r sprintf("%.3f", icc_feature_all %>% filter(Group=="Expert") %>% slice_min(ICC, n = 1) %>% pull(ICC))`).
Experts show very low consistency overall, with especially poor agreement on **Conciseness** (even negative ICC).

* **Auto‐graders**
  
  * **Overall** ICC = `r sprintf("%.3f", icc_overall$icc[icc_overall$group=="Auto-grader"])`.
* **Highest**: `r icc_feature_all %>% filter(Group=="Auto-grader") %>% slice_max(ICC, n = 1) %>% pull(Feature)`
(ICC = `r sprintf("%.3f", icc_feature_all %>% filter(Group=="Auto-grader") %>% slice_max(ICC, n = 1) %>% pull(ICC))`).
* **Lowest**: `r icc_feature_all %>% filter(Group=="Auto-grader") %>% slice_min(ICC, n = 1) %>% pull(Feature)`
(ICC = `r sprintf("%.3f", icc_feature_all %>% filter(Group=="Auto-grader") %>% slice_min(ICC, n = 1) %>% pull(ICC))`).
Auto‐graders are most consistent on **Completeness** and least on **Readability**.

> **Take‐away:**
  > Breaking ICC down by feature reveals specific dimensions where raters excel or falter, guiding rubric improvements and training priorities.



### 3.2 Between-Group Reliability

```{r between-group-icc}
# Calculate ICC for different group combinations
calculate_between_group_icc <- function(data, group1_raters, group2_raters, feature_name, group_label) {
  all_raters <- c(group1_raters, group2_raters)

  icc_data <- data %>%
    filter(rater_name %in% all_raters, feature == feature_name) %>%
    select(query_id, rater_name, rating) %>%
    pivot_wider(names_from = rater_name, values_from = rating) %>%
    select(-query_id) %>%
    as.matrix()

  if (ncol(icc_data) > 1 && nrow(icc_data) > 1) {
    icc_result <- ICC(icc_data)
    return(data.frame(
      Feature = feature_name,
      Comparison = group_label,
      ICC2k = icc_result$results["ICC2k", "ICC"],
      ICC2k_lower = icc_result$results["ICC2k", "lower bound"],
      ICC2k_upper = icc_result$results["ICC2k", "upper bound"],
      p_value = icc_result$results["ICC2k", "p"]
    ))
  } else {
    return(data.frame(
      Feature = feature_name,
      Comparison = group_label,
      ICC2k = NA,
      ICC2k_lower = NA,
      ICC2k_upper = NA,
      p_value = NA
    ))
  }
}

# Calculate between-group ICCs
between_group_results <- list()

# Students vs Experts
between_group_results$stud_exp <- map_df(
  rating_features,
  ~ calculate_between_group_icc(df_long, student_raters, expert_raters, .x, "Students vs Experts")
)

# Human (all) vs Auto
human_raters <- c(student_raters, expert_raters)
between_group_results$human_auto <- map_df(
  rating_features,
  ~ calculate_between_group_icc(df_long, human_raters, auto_raters, .x, "Human vs Auto")
)

# Experts vs Auto
between_group_results$exp_auto <- map_df(
  rating_features,
  ~ calculate_between_group_icc(df_long, expert_raters, auto_raters, .x, "Experts vs Auto")
)

# All raters combined
all_raters <- c(student_raters, expert_raters, auto_raters)
between_group_results$all <- map_df(
  rating_features,
  ~ calculate_between_group_icc(df_long, all_raters, character(0), .x, "All Raters")
)

# Combine results
between_icc_combined <- bind_rows(between_group_results)

# Create visualization
p_icc_between <- ggplot(
  between_icc_combined %>%
    filter(Comparison != "All Raters"),
  aes(x = Feature, y = ICC2k, color = Comparison, group = Comparison)
) +
  geom_point(size = 3, position = position_dodge(width = 0.3)) +
  geom_errorbar(aes(ymin = ICC2k_lower, ymax = ICC2k_upper),
    width = 0.2, position = position_dodge(width = 0.3)
  ) +
  geom_hline(
    yintercept = 0.6, linetype = "dashed", color = brand_colors$alert_red,
    size = 1, alpha = 0.7
  ) +
  scale_color_manual(values = c(
    "Students vs Experts" = brand_colors$primary_navy,
    "Human vs Auto" = brand_colors$supporting_blue,
    "Experts vs Auto" = brand_colors$accent_yellow
  )) +
  labs(
    title = "Between-Group Inter-Rater Reliability",
    subtitle = "ICC(2,k) for different rater group combinations",
    x = "Feature",
    y = "ICC(2,k)",
    color = "Group Comparison"
  ) +
  theme(axis.text.x = element_text(angle = 45, hjust = 1)) +
  coord_cartesian(ylim = c(0, 1))

print(p_icc_between)
```

## 4. Model Performance Comparison

### 4.1 Linear Mixed-Effects Models

```{r lmm-analysis}
# Function to fit LMM for each feature
fit_feature_lmm <- function(data, feature_name) {
  feature_data <- data %>%
    filter(feature == feature_name)

  # Fit the model with random intercepts for question and rater
  model <- lmer(rating ~ model + (1 | query_id) + (1 | rater_name),
    data = feature_data, REML = FALSE
  )

  # Get model summary
  model_summary <- summary(model)

  # Perform likelihood ratio test
  null_model <- lmer(rating ~ 1 + (1 | query_id) + (1 | rater_name),
    data = feature_data, REML = FALSE
  )
  lr_test <- anova(null_model, model)

  # Get estimated marginal means
  emm <- emmeans(model, ~model)
  emm_df <- as.data.frame(emm)

  # Perform pairwise comparisons
  pairs <- pairs(emm, adjust = "tukey")

  return(list(
    feature = feature_name,
    model = model,
    summary = model_summary,
    lr_test = lr_test,
    emmeans = emm_df,
    pairwise = pairs
  ))
}

# Fit models for all features
lmm_results <- map(rating_features, ~ fit_feature_lmm(df_long, .x))
names(lmm_results) <- rating_features

# Extract key statistics for each feature
feature_model_stats <- map_df(lmm_results, function(res) {
  lr_p <- res$lr_test$`Pr(>Chisq)`[2]
  data.frame(
    Feature = res$feature,
    LR_Chisq = res$lr_test$Chisq[2],
    df = res$lr_test$Df[2],
    p_value = lr_p,
    Significant = ifelse(lr_p < 0.001, "***",
      ifelse(lr_p < 0.01, "**",
        ifelse(lr_p < 0.05, "*", "ns")
      )
    )
  )
})

# Create estimated marginal means plot for significant features
significant_features <- feature_model_stats %>%
  filter(p_value < 0.05) %>%
  pull(Feature)

if (length(significant_features) > 0) {
  emm_data <- map_df(significant_features, function(feat) {
    lmm_results[[feat]]$emmeans %>%
      mutate(Feature = feat)
  })

  p_emm <- ggplot(emm_data, aes(x = model, y = emmean, color = Feature)) +
    geom_point(position = position_dodge(width = 0.5), size = 3) +
    geom_errorbar(aes(ymin = lower.CL, ymax = upper.CL),
      position = position_dodge(width = 0.5), width = 0.3
    ) +
    facet_wrap(~Feature, scales = "free_x", ncol = 3) +
    labs(
      title = "Estimated Marginal Means by Model",
      subtitle = "Features with significant model differences (p < 0.05)",
      x = "Model",
      y = "Estimated Mean Rating",
      color = "Feature"
    ) +
    theme(
      axis.text.x = element_text(angle = 45, hjust = 1),
      legend.position = "none"
    )

  print(p_emm)
}

# Create summary table
kable(feature_model_stats,
  caption = "Table 4. Likelihood ratio tests for model effects by feature",
  format = "latex",
  booktabs = TRUE,
  digits = c(0, 2, 0, 4, 0)
) %>%
  kable_styling(latex_options = c("striped", "hold_position"))
```

Pr(>Chisq)`[2]
  ))
}

# Fit models for all features
rater_type_results <- map(rating_features, ~fit_rater_type_lmm(df_long, .x))
names(rater_type_results) <- rating_features

# Extract rater type main effects
rater_type_stats <- map_df(rater_type_results, function(res) {
  # Get estimated marginal means for rater types
  emm <- emmeans(res$model_no_int, ~ rater_type_detailed)
  emm_df <- as.data.frame(emm)
  
  # Perform pairwise comparisons
  pairs <- pairs(emm, adjust = "tukey")
  pairs_df <- as.data.frame(pairs)
  
  # Create summary
  data.frame(
    Feature = res$feature,
    Student_Mean = emm_df$emmean[emm_df$rater_type_detailed == "Student"],
    Expert_Mean = emm_df$emmean[emm_df$rater_type_detailed == "Expert"],
    Auto_Mean = emm_df$emmean[emm_df$rater_type_detailed == "Auto-grader"],
    Interaction_p = res$interaction_p,
    Student_Expert_p = pairs_df$p.value[pairs_df$contrast == "Student - Expert"],
    Student_Auto_p = pairs_df$p.value[pairs_df$contrast == "Student - Auto-grader"],
    Expert_Auto_p = pairs_df$p.value[pairs_df$contrast == "Expert - Auto-grader"]
  )
})

# Create visualization for rater type differences
p_rater_differences <- rater_type_stats %>%
  pivot_longer(cols = c(Student_Mean, Expert_Mean, Auto_Mean),
               names_to = "Rater_Type",
               values_to = "Mean_Rating") %>%
  mutate(Rater_Type = str_remove(Rater_Type, "_Mean")) %>%
  ggplot(aes(x = Feature, y = Mean_Rating, color = Rater_Type, group = Rater_Type)) +
  geom_point(size = 3, position = position_dodge(width = 0.3)) +
  geom_line(position = position_dodge(width = 0.3), alpha = 0.5) +
  scale_color_manual(values = c("Student" = brand_colors$supporting_blue,
                               "Expert" = brand_colors$primary_navy,
                               "Auto" = brand_colors$accent_yellow)) +
  labs(
    title = "Rater Type Effects Across Features",
    subtitle = "Estimated marginal means from mixed-effects models",
    x = "Feature",
    y = "Mean Rating",
    color = "Rater Type"
  ) +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))

print(p_rater_differences)

# Create summary table
rater_type_table <- rater_type_stats %>%
  mutate(
    Student = sprintf("%.2f", Student_Mean),
    Expert = sprintf("%.2f", Expert_Mean),
    `Auto-grader` = sprintf("%.2f", Auto_Mean),
    `S-E` = ifelse(Student_Expert_p < 0.001, "***",
                   ifelse(Student_Expert_p < 0.01, "**",
                         ifelse(Student_Expert_p < 0.05, "*", "ns"))),
    `S-A` = ifelse(Student_Auto_p < 0.001, "***",
                   ifelse(Student_Auto_p < 0.01, "**",
                         ifelse(Student_Auto_p < 0.05, "*", "ns"))),
    `E-A` = ifelse(Expert_Auto_p < 0.001, "***",
                   ifelse(Expert_Auto_p < 0.01, "**",
                         ifelse(Expert_Auto_p < 0.05, "*", "ns")))
  ) %>%
  select(Feature, Student, Expert, `Auto-grader`, `S-E`, `S-A`, `E-A`)

kable(rater_type_table, 
      caption = "Table 5. Rater type effects and pairwise comparisons",
      format = "latex", 
      booktabs = TRUE) %>%
  kable_styling(latex_options = c("striped", "hold_position")) %>%
  add_header_above(c(" " = 4, "Pairwise Comparisons" = 3))
```

## 5. Multivariate Analysis

### 5.1 Principal Component Analysis

```{r pca-analysis}
# Prepare data for PCA - average ratings by model and feature
pca_data <- df_long %>%
  group_by(model, feature) %>%
  summarise(mean_rating = mean(rating, na.rm = TRUE), .groups = "drop") %>%
  pivot_wider(names_from = feature, values_from = mean_rating) %>%
  column_to_rownames("model")

# Perform PCA
pca_result <- prcomp(pca_data, scale = TRUE, center = TRUE)

# Calculate variance explained
var_explained <- summary(pca_result)$importance[2,] * 100
cumvar_explained <- summary(pca_result)$importance[3,] * 100

# Create scree plot
scree_data <- data.frame(
  PC = 1:length(var_explained),
  Variance = var_explained,
  Cumulative = cumvar_explained
)

p_scree <- ggplot(scree_data, aes(x = PC)) +
  geom_bar(aes(y = Variance), stat = "identity", fill = brand_colors$primary_navy) +
  geom_line(aes(y = Cumulative), color = brand_colors$alert_red, size = 1.5) +
  geom_point(aes(y = Cumulative), color = brand_colors$alert_red, size = 3) +
  geom_hline(yintercept = 80, linetype = "dashed", color = brand_colors$accent_yellow) +
  scale_x_continuous(breaks = 1:9) +
  labs(
    title = "PCA Scree Plot",
    subtitle = "Variance explained by principal components",
    x = "Principal Component",
    y = "Variance Explained (%)"
  )

print(p_scree)

# Create biplot
fviz_pca_biplot(pca_result, 
                col.var = brand_colors$primary_navy,
                col.ind = brand_colors$accent_yellow,
                label = "all",
                labelsize = 4,
                pointsize = 3,
                repel = TRUE) +
  labs(title = "PCA Biplot: Models and Features",
       subtitle = "First two principal components") +
  theme_minimal()

# Feature loadings
loadings_df <- as.data.frame(pca_result$rotation[, 1:2])
loadings_df$feature <- rownames(loadings_df)

p_loadings <- ggplot(loadings_df, aes(x = PC1, y = PC2, label = feature)) +
  geom_segment(aes(x = 0, y = 0, xend = PC1, yend = PC2), 
               arrow = arrow(length = unit(0.3, "cm")),
               color = brand_colors$supporting_blue) +
  geom_text_repel(size = 4, color = brand_colors$primary_navy) +
  geom_hline(yintercept = 0, linetype = "dashed", alpha = 0.5) +
  geom_vline(xintercept = 0, linetype = "dashed", alpha = 0.5) +
  labs(
    title = "Feature Loadings on First Two Principal Components",
    x = sprintf("PC1 (%.1f%% variance)", var_explained[1]),
    y = sprintf("PC2 (%.1f%% variance)", var_explained[2])
  ) +
  coord_equal()

print(p_loadings)

# Create loadings table
loadings_table <- loadings_df %>%
  arrange(desc(abs(PC1))) %>%
  mutate(
    PC1 = sprintf("%.3f", PC1),
    PC2 = sprintf("%.3f", PC2)
  ) %>%
  select(Feature = feature, PC1, PC2)

kable(loadings_table, 
      caption = "Table 6. Feature loadings on first two principal components",
      format = "latex", 
      booktabs = TRUE) %>%
  kable_styling(latex_options = c("striped", "hold_position"))
```

### 5.2 Cluster Analysis of Models

```{r cluster-analysis}
# Perform hierarchical clustering
dist_matrix <- dist(pca_data, method = "euclidean")
hclust_result <- hclust(dist_matrix, method = "ward.D2")

# Convert to dendrogram
dend <- as.dendrogram(hclust_result)
dend <- color_branches(dend, k = 3, col = c(
  brand_colors$primary_navy,
  brand_colors$accent_yellow,
  brand_colors$supporting_blue
))

# Create dendrogram plot
p_dendrogram <- ggplot(as.ggdend(dend)) +
  labs(
    title = "Hierarchical Clustering of Models",
    subtitle = "Based on average ratings across all features",
    x = "Model",
    y = "Distance"
  ) +
  theme(axis.text.x = element_text(angle = 0, hjust = 0.5))

print(p_dendrogram)

# K-means clustering
set.seed(123)
kmeans_result <- kmeans(pca_data, centers = 3, nstart = 25)

# Add cluster assignments to PCA plot
pca_scores <- as.data.frame(pca_result$x[, 1:2])
pca_scores$model <- rownames(pca_scores)
pca_scores$cluster <- factor(kmeans_result$cluster)

p_clusters <- ggplot(pca_scores, aes(x = PC1, y = PC2, color = cluster, label = model)) +
  geom_point(size = 4) +
  geom_text_repel(size = 4) +
  scale_color_manual(values = c(
    brand_colors$primary_navy,
    brand_colors$accent_yellow,
    brand_colors$supporting_blue
  )) +
  labs(
    title = "Model Clusters in PCA Space",
    subtitle = "K-means clustering (k=3)",
    x = sprintf("PC1 (%.1f%% variance)", var_explained[1]),
    y = sprintf("PC2 (%.1f%% variance)", var_explained[2]),
    color = "Cluster"
  )

print(p_clusters)
```

### 5.3 Auto-grader vs Human Consensus Analysis

```{r auto-human-comparison}
# Calculate human consensus scores
human_consensus <- df_long %>%
  filter(grader_type == "human") %>%
  group_by(query_id, model, feature) %>%
  summarise(human_mean = mean(rating, na.rm = TRUE), .groups = "drop")

# Calculate individual auto-grader correlations with human consensus
auto_correlations <- df_long %>%
  filter(grader_type == "auto") %>%
  left_join(human_consensus, by = c("query_id", "model", "feature")) %>%
  group_by(rater_name, feature) %>%
  summarise(
    correlation = cor(rating, human_mean, use = "complete.obs"),
    n_pairs = sum(!is.na(rating) & !is.na(human_mean)),
    .groups = "drop"
  )

# Create heatmap of correlations
correlation_matrix <- auto_correlations %>%
  pivot_wider(names_from = feature, values_from = correlation) %>%
  column_to_rownames("rater_name") %>%
  as.matrix()

# Create heatmap
p_auto_human_corr <- correlation_matrix %>%
  as.data.frame() %>%
  rownames_to_column("Auto_Grader") %>%
  pivot_longer(-Auto_Grader, names_to = "Feature", values_to = "Correlation") %>%
  mutate(
    Auto_Grader = str_remove(Auto_Grader, "autoLLM_"),
    Feature = factor(Feature, levels = rating_features)
  ) %>%
  ggplot(aes(x = Feature, y = Auto_Grader, fill = Correlation)) +
  geom_tile(color = "white", size = 0.5) +
  geom_text(aes(label = sprintf("%.2f", Correlation)), size = 3) +
  scale_fill_gradient2(
    low = brand_colors$alert_red,
    mid = "white",
    high = brand_colors$success_green,
    midpoint = 0.7,
    limits = c(0.4, 1),
    name = "Correlation"
  ) +
  labs(
    title = "Auto-grader Agreement with Human Consensus",
    subtitle = "Pearson correlation coefficients",
    x = "Feature",
    y = "Auto-grader"
  ) +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))

print(p_auto_human_corr)

# Calculate RMSE and bias for each auto-grader
auto_performance <- df_long %>%
  filter(grader_type == "auto") %>%
  left_join(human_consensus, by = c("query_id", "model", "feature")) %>%
  group_by(rater_name) %>%
  summarise(
    RMSE = sqrt(mean((rating - human_mean)^2, na.rm = TRUE)),
    MAE = mean(abs(rating - human_mean), na.rm = TRUE),
    Bias = mean(rating - human_mean, na.rm = TRUE),
    Correlation = cor(rating, human_mean, use = "complete.obs"),
    .groups = "drop"
  ) %>%
  mutate(rater_name = str_remove(rater_name, "autoLLM_"))

# Create performance comparison plot
p_auto_performance <- auto_performance %>%
  pivot_longer(
    cols = c(RMSE, MAE, Bias, Correlation),
    names_to = "Metric",
    values_to = "Value"
  ) %>%
  ggplot(aes(x = rater_name, y = Value, fill = Metric)) +
  geom_bar(stat = "identity", position = "dodge") +
  facet_wrap(~Metric, scales = "free_y", ncol = 2) +
  scale_fill_manual(values = c(
    "RMSE" = brand_colors$alert_red,
    "MAE" = brand_colors$accent_yellow,
    "Bias" = brand_colors$supporting_blue,
    "Correlation" = brand_colors$success_green
  )) +
  labs(
    title = "Auto-grader Performance Metrics",
    subtitle = "Comparison with human consensus ratings",
    x = "Auto-grader",
    y = "Value"
  ) +
  theme(
    axis.text.x = element_text(angle = 45, hjust = 1),
    legend.position = "none"
  )

print(p_auto_performance)

# Create summary table
auto_performance_table <- auto_performance %>%
  mutate(
    RMSE = sprintf("%.3f", RMSE),
    MAE = sprintf("%.3f", MAE),
    Bias = sprintf("%.3f", Bias),
    Correlation = sprintf("%.3f", Correlation)
  ) %>%
  rename(`Auto-grader` = rater_name)

kable(auto_performance_table,
  caption = "Table 7. Auto-grader performance metrics compared to human consensus",
  format = "latex",
  booktabs = TRUE
) %>%
  kable_styling(latex_options = c("striped", "hold_position"))
```

## 6. Summary and Key Findings

### 6.1 Overall Performance Summary

```{r summary-visualization}
# Create comprehensive summary plot
# Combine key findings into a single publication-ready figure

# Panel A: Model rankings
panel_a <- df_long %>%
  group_by(model) %>%
  summarise(
    mean_rating = mean(rating, na.rm = TRUE),
    se = sd(rating, na.rm = TRUE) / sqrt(n()),
    .groups = "drop"
  ) %>%
  ggplot(aes(x = reorder(model, mean_rating), y = mean_rating)) +
  geom_bar(stat = "identity", fill = brand_colors$primary_navy) +
  geom_errorbar(aes(ymin = mean_rating - se, ymax = mean_rating + se),
    width = 0.3
  ) +
  coord_flip() +
  labs(
    title = "A. Overall Model Performance",
    x = "Model", y = "Mean Rating"
  ) +
  theme_minimal(base_size = 10)

# Panel B: Feature reliability
panel_b <- icc_combined %>%
  filter(Group == "All Raters") %>%
  ggplot(aes(x = reorder(Feature, ICC2k), y = ICC2k)) +
  geom_bar(stat = "identity", fill = brand_colors$supporting_blue) +
  geom_hline(yintercept = 0.6, linetype = "dashed", color = brand_colors$alert_red) +
  coord_flip() +
  labs(
    title = "B. Feature Reliability (All Raters)",
    x = "Feature", y = "ICC(2,k)"
  ) +
  theme_minimal(base_size = 10)

# Panel C: Rater type agreement
panel_c <- df_long %>%
  group_by(rater_type_detailed) %>%
  summarise(
    mean_rating = mean(rating, na.rm = TRUE),
    sd = sd(rating, na.rm = TRUE),
    .groups = "drop"
  ) %>%
  ggplot(aes(x = rater_type_detailed, y = mean_rating, fill = rater_type_detailed)) +
  geom_bar(stat = "identity") +
  geom_errorbar(aes(ymin = mean_rating - sd, ymax = mean_rating + sd),
    width = 0.3
  ) +
  scale_fill_manual(values = c(
    "Student" = brand_colors$supporting_blue,
    "Expert" = brand_colors$primary_navy,
    "Auto-grader" = brand_colors$accent_yellow
  )) +
  labs(
    title = "C. Rater Type Comparison",
    x = "Rater Type", y = "Mean Rating"
  ) +
  theme_minimal(base_size = 10) +
  theme(legend.position = "none")

# Panel D: Auto-grader accuracy
panel_d <- auto_performance %>%
  ggplot(aes(x = reorder(rater_name, -Correlation), y = Correlation)) +
  geom_bar(stat = "identity", fill = brand_colors$success_green) +
  geom_hline(yintercept = 0.8, linetype = "dashed", color = brand_colors$alert_red) +
  labs(
    title = "D. Auto-grader Correlation with Humans",
    x = "Auto-grader", y = "Correlation"
  ) +
  theme_minimal(base_size = 10) +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))

# Combine panels
summary_plot <- (panel_a | panel_b) / (panel_c | panel_d) +
  plot_annotation(
    title = "Figure 8. Summary of Key Findings",
    subtitle = "Comprehensive overview of model performance, feature reliability, and rater agreement"
  )

print(summary_plot)

# Save for publication
ggsave("figures/figure8_summary_findings.pdf", summary_plot, width = 12, height = 10)
```

### 6.2 Key Statistical Findings

```{r key-findings-table}
# Create summary table of key findings
key_findings <- data.frame(
  Metric = c(
    "Best Performing Model",
    "Most Reliable Feature",
    "Least Reliable Feature",
    "Human-Auto Agreement",
    "Expert-Student Agreement",
    "Features with Significant Model Differences"
  ),
  Finding = c(
    model_performance %>% arrange(desc(mean_rating)) %>% slice(1) %>% pull(model),
    feature_performance %>% arrange(desc(mean_rating)) %>% slice(1) %>% pull(feature),
    feature_performance %>% arrange(mean_rating) %>% slice(1) %>% pull(feature),
    sprintf("r = %.3f", mean(auto_performance$Correlation)),
    sprintf("ICC = %.3f", mean(between_icc_combined %>%
      filter(Comparison == "Students vs Experts") %>%
      pull(ICC2k), na.rm = TRUE)),
    sum(feature_model_stats$p_value < 0.05)
  ),
  Details = c(
    sprintf(
      "Mean rating = %.2f",
      model_performance %>% arrange(desc(mean_rating)) %>% slice(1) %>% pull(mean_rating)
    ),
    sprintf(
      "Mean rating = %.2f",
      feature_performance %>% arrange(desc(mean_rating)) %>% slice(1) %>% pull(mean_rating)
    ),
    sprintf(
      "Mean rating = %.2f",
      feature_performance %>% arrange(mean_rating) %>% slice(1) %>% pull(mean_rating)
    ),
    "Average across all auto-graders",
    "Average across all features",
    paste(significant_features, collapse = ", ")
  )
)

kable(key_findings,
  caption = "Table 8. Summary of key statistical findings",
  format = "latex",
  booktabs = TRUE
) %>%
  kable_styling(latex_options = c("striped", "hold_position")) %>%
  column_spec(1, width = "5cm") %>%
  column_spec(3, width = "6cm")
```

## 7. Supplementary Materials

### 7.1 Model Diagnostics

```{r model-diagnostics}
# Check assumptions for a representative LMM
representative_model <- lmm_results[["Appropriateness"]]$model

# Create diagnostic plots
p_diagnostics <- plot(check_model(representative_model),
  colors = c(brand_colors$primary_navy, brand_colors$alert_red)
)

# QQ plot of random effects
ranef_df <- as.data.frame(ranef(representative_model))

p_qq_random <- ggplot(ranef_df$query_id, aes(sample = `(Intercept)`)) +
  stat_qq() +
  stat_qq_line(color = brand_colors$alert_red) +
  labs(
    title = "QQ Plot of Random Effects (Query ID)",
    x = "Theoretical Quantiles",
    y = "Sample Quantiles"
  ) +
  theme_minimal()

print(p_qq_random)
```

### 7.2 Complete Pairwise Comparisons

```{r pairwise-full}
# Generate complete pairwise comparison tables for significant features
if (length(significant_features) > 0) {
  pairwise_tables <- map(significant_features, function(feat) {
    pairs_df <- as.data.frame(lmm_results[[feat]]$pairwise)
    pairs_df %>%
      mutate(
        Feature = feat,
        Estimate = sprintf("%.3f", estimate),
        SE = sprintf("%.3f", SE),
        `95% CI` = sprintf("[%.3f, %.3f]", lower.CL, upper.CL),
        p_adj = sprintf("%.4f", p.value),
        Sig = ifelse(p.value < 0.001, "***",
          ifelse(p.value < 0.01, "**",
            ifelse(p.value < 0.05, "*", "")
          )
        )
      ) %>%
      select(Feature, Contrast = contrast, Estimate, SE, `95% CI`, `p-value` = p_adj, Sig)
  })

  # Combine all pairwise comparisons
  all_pairwise <- bind_rows(pairwise_tables)

  # Create supplementary table
  kable(all_pairwise %>% filter(Feature == significant_features[1]),
    caption = sprintf("Table S1. Pairwise model comparisons for %s", significant_features[1]),
    format = "latex",
    booktabs = TRUE
  ) %>%
    kable_styling(latex_options = c("striped", "hold_position", "scale_down"))
}
```

## References

```{r save-session-info}
# Save session information for reproducibility
sink("session_info.txt")
sessionInfo()
sink()

# Print completion message
cat("\nAnalysis completed successfully. All figures saved to 'figures/' directory.\n")
```