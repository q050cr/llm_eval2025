---
title: "LLM Performance in Clinical Question Answering: Results"
author: "Christoph Reich"
date: "`r Sys.Date()`"
format:
  html:
    toc: true
    toc-depth: 3
    number-sections: false
    echo: false
    #code-fold: true
    #code-tools: true
    #code-summary: "Show the code"
    theme: cosmo
    fig-width: 10
    fig-height: 8
  pdf:
    toc: true
    number-sections: false
    colorlinks: true
    echo: false
    fig-width: 7
    fig-height: 5
    geometry:  # Set margins for PDF output default geometry: margin=1in (25.4mm all sides)
      - top=20mm
      - left=15mm
      - right=15mm
      - bottom=20mm
    mainfont: Times New Roman
    documentclass: report 
    classoption: [11pt] # Font size
    keep-tex: true # Keep the intermediate .tex file for debugging
---

```{r setup, include=FALSE}
#| label: Load-required-libraries
library(tidyverse)
library(knitr)
library(kableExtra)
library(gt)
library(ggplot2)
library(patchwork)
library(quanteda)
library(quanteda.textstats)
library(lme4)
library(lmerTest)
library(emmeans)
library(irr)
library(psych)
library(corrplot)
library(viridis)
library(ggsignif)
library(ggrepel)
library(factoextra)
library(dendextend)
library(quanteda)
library(cowplot)
library(broom.mixed)
library(performance)
library(see)

# Set global options
knitr::opts_chunk$set(
  # echo = TRUE,
  warning = FALSE,
  message = FALSE,
  fig.width = 10,
  fig.height = 8,
  dpi = 300
)

# Set theme for all plots
theme_set(
  theme_minimal(base_size = 16) +
    theme(
      panel.grid.minor = element_blank(),
      plot.title = element_text(size = 18, face = "bold"),
      axis.title = element_text(size = 16),
      legend.position = "bottom"
    )
)

# Define color palettes based on the visualization playbook
brand_colors <- list(
  primary_navy = "#2C4B73",
  accent_yellow = "#F4C430",
  supporting_blue = "#7BA7BC",
  light_gray = "#E8E8E8",
  white = "#FFFFFF",
  alert_red = "#D73027",
  success_green = "#1A9850"
)

# AI company brand colors
ai_colors <- c(
  "openai" = "#17A683",
  "google" = "#528DD5",
  "deepseek" = "#5370FE",
  "perplexity" = "#278491",
  "anthropic" = "#D5A583",
  "xai" = "#000000"
)

# Define rater groups
student_raters <- c("Isabel_LLM", "Jule_LLM", "Lasse_LLM")
expert_raters <- c("Christoph_LLM", "Musti_LLM", "Charlotte_LLM")
auto_raters <- c(
  "autoLLM_gemini_2.5_pro",
  "autoLLM_gemini_2.0_flash",
  "autoLLM_anthropic_somnet3.5",
  "autoLLM_anthropic_somnet4",
  "autoLLM_openai_4o_strict",
  "autoLLM_openai_4o"
)

# Define rating features
rating_features <- c(
  "Appropriateness",
  "Comprehensibility",
  "Completeness",
  "Conciseness",
  "Confabulation Avoidance",
  "Readability",
  "Educational Value",
  "Actionability",
  "Tone/Empathy"
)
```

```{r}
#| label: pdf-table-helper-function

# Helper function to format gt tables for PDF output
format_gt_for_pdf <- function(gt_table, font_size = 8, width_pct = 95) {
  if (knitr::is_latex_output()) {
    gt_table %>%
      tab_options(
        table.font.size = font_size,
        table.width = pct(width_pct),
        column_labels.font.size = font_size + 1,
        row_group.font.size = font_size + 1,
        footnotes.font.size = font_size - 1,
        source_notes.font.size = font_size - 1
      ) %>%
      cols_width(everything() ~ px(80)) # Adjust column widths
  } else {
    gt_table
  }
}
```

```{r load-data}
#| label: Load-data
df <- read_csv(
  "./data/data_preparation/individual_ratings_20250606_115527.csv",
  show_col_types = FALSE
)

# Create rater type categories
df <- df %>%
  mutate(
    rater_type_detailed = case_when(
      rater_name %in% student_raters ~ "Student",
      rater_name %in% expert_raters ~ "Expert",
      rater_name %in% auto_raters ~ "Auto-grader",
      TRUE ~ "Unknown"
    ),
    human_vs_auto = ifelse(grader_type == "human", "Human", "Automated")
  )

# Reshape data to long format for easier analysis
df_long <- df %>%
  pivot_longer(
    cols = all_of(rating_features),
    names_to = "feature",
    values_to = "rating"
  ) %>%
  mutate(
    feature = factor(feature, levels = rating_features),
    model = factor(model),
    rater_name = factor(rater_name)
  )
```


# Results

## 1. Descriptive Statistics

### 1.1 Response Characteristics

```{r response-readability}
#| label: unique-responses

# Prepare unique responses: collapse across raters to one per query/model
unique_responses <- df %>%
  distinct(index, model, category, response) %>%
  # Set category factor order
  mutate(
    category = factor(
      category,
      levels = c(
        "Disease Understanding and Diagnosis",
        "Treatment and Management",
        "Lifestyle & Daily Activity"
      )
    )
  )
```


We calculate and present readability metrics for medical text responses using the **quanteda** and **quanteda.textstats** packages. We will compute the following indices:
  
  * **Flesch Reading Ease**
  * **Flesch-Kincaid Grade Level**
  * **Gunning Fog Index**
  * **SMOG Index**
  * **Coleman-Liau Index**
  * **Automated Readability Index (ARI)**
  
  We will present descriptive statistics and visualizations in two ways:
  
  1. **Per model**
  2. **Per model and category**
  

#### A) Flesch-Based Readability Formulas

1. **Flesch Reading Ease (FRE)**

   * **Formula**: `206.835 – (1.015 × ASL) – (84.6 × ASW)`

     * *ASL*: Average Sentence Length (words per sentence)
     * *ASW*: Average Syllables per Word
   * **Output**: Score ranging from 0 to 100; higher scores indicate easier readability.
   * **Interpretation**:

     * 90–100: Very easy (understood by 11-year-olds)
     * 60–70: Standard (13–15-year-olds)
     * 30–50: Difficult (college level)
   * **Usage**: Provides a general sense of text readability but requires a conversion table to relate scores to grade levels. ([en.wikipedia.org][1], [readable.com][2])

2. **Flesch-Kincaid Grade Level (FKGL)**

   * **Formula**: `0.39 × ASL + 11.8 × ASW – 15.59`
   * **Output**: U.S. school grade level; for example, a score of 8.5 corresponds to an 8th-grade reading level.
   * **Usage**: Directly indicates the education level required to comprehend the text, making it practical for assessing materials intended for specific audiences. ([de.wikipedia.org][3], [nira.com][4])

3. **Flesch.PSK (Powers-Sumner-Kearl Variation)**

   * **Formula**: `(0.0778 × ASL) + (4.55 × ASW) – 2.2029`
   * **Usage**: A less commonly used variation; not widely adopted in current readability assessments.

---

In medical and health-related fields, ensuring that patient education materials are accessible is crucial. Both FRE and FKGL are commonly used to assess the readability of such materials. However, FKGL is often preferred because it provides a direct correlation to U.S. grade levels, simplifying the assessment of whether materials are appropriate for the target audience. ([journals.lww.com](https://journals.lww.com/edhe/fulltext/2017/30010/assessing_reading_levels_of_health_information_.15.aspx))

For instance, a study analyzing patient information sheets found that the mean FKGL was 11.4, indicating that the materials were written at a level suitable for individuals with at least an 11th-grade education. ([pubmed.ncbi.nlm.nih.gov](https://pubmed.ncbi.nlm.nih.gov/36131293/))


* **Use Flesch-Kincaid Grade Level (FKGL)** when you need a straightforward indication of the education level required to understand the text. This is particularly useful in medical contexts, where materials should be tailored to the patient's reading ability.([en.wikipedia.org][7])
* **Use Flesch Reading Ease (FRE)** if you prefer a score that reflects the overall readability on a 100-point scale. This can be helpful for comparing the readability of different texts or versions of the same text.


```{r}
#| label: fn-readability-metrics

clean_text_for_readability <- function(text) {
  text %>%
    # Remove markdown formatting
    str_replace_all("\\*\\*(.*?)\\*\\*", "\\1") %>% # Remove **bold**
    str_replace_all("###\\s*", "") %>% # Remove ### headers
    str_replace_all("#\\s*", "") %>% # Remove # headers
    # Clean up special characters that might confuse readability
    str_replace_all("≤", "less than or equal to") %>%
    # Remove extra whitespace
    str_replace_all("\\s+", " ") %>%
    str_trim()
}

# Modified function
compute_readability <- function(texts) {
  cleaned_texts <- map_chr(texts, clean_text_for_readability)
  txt_quanteda <- quanteda::corpus(cleaned_texts)
  stats <- quanteda.textstats::textstat_readability(
    txt_quanteda,
    measure = c(
      "Flesch",
      "Flesch.Kincaid",
      "FOG",
      "SMOG",
      "Coleman.Liau.short",
      "ARI"
    )
  )
  as_tibble(stats)
}

# Original function (no cleaning)
compute_readability_raw <- function(texts) {
  txt_quanteda <- quanteda::corpus(texts) # No cleaning step
  stats <- quanteda.textstats::textstat_readability(
    txt_quanteda,
    measure = c(
      "Flesch",
      "Flesch.Kincaid",
      "FOG",
      "SMOG",
      "Coleman.Liau.short",
      "ARI"
    )
  )
  as_tibble(stats)
}
```


```{r}
#| label: compare-cleaned-text-stats-with-raw
#| include: false

# Test on a few more samples to see if the pattern holds
test_samples <- head(unique_responses$response, 5) # Test first 5 responses

# Compare raw vs cleaned across multiple texts
comparison_results <- map_dfr(
  1:length(test_samples),
  ~ {
    text <- test_samples[.x]

    raw <- compute_readability_raw(text) %>%
      mutate(version = "raw", sample = .x)
    cleaned <- compute_readability(clean_text_for_readability(text)) %>%
      mutate(version = "cleaned", sample = .x)

    bind_rows(raw, cleaned)
  }
)

# Check if any differences exist
res_cleaning <- comparison_results %>%
  select(-document) %>%
  pivot_wider(names_from = version, values_from = c(Flesch:ARI)) %>%
  mutate(
    flesch_diff = abs(Flesch_raw - Flesch_cleaned),
    fk_diff = abs(Flesch.Kincaid_raw - Flesch.Kincaid_cleaned)
  )

# no diffs observed
```

```{r}
#| label: readability-for-each-response-row-wise

readability_scores <- unique_responses %>%
  mutate(
    rid = row_number(),
    .metrics = compute_readability(response)
  ) %>%
  unnest(cols = c(.metrics))
```


```{r}
#| label: Readability-per-model

read_by_model <- readability_scores %>%
  group_by(model) %>%
  summarise(
    # Count total responses
    n_responses = n(),
    `Flesch Ease` = sprintf(
      "%.1f ± %.1f",
      mean(Flesch, na.rm = TRUE),
      sd(Flesch, na.rm = TRUE)
    ),
    `Flesch K-G` = sprintf(
      "%.1f ± %.1f",
      mean(`Flesch.Kincaid`, na.rm = TRUE),
      sd(`Flesch.Kincaid`, na.rm = TRUE)
    ),
    `Gunning Fog` = sprintf(
      "%.1f ± %.1f",
      mean(FOG, na.rm = TRUE),
      sd(FOG, na.rm = TRUE)
    ),
    SMOG = sprintf(
      "%.1f ± %.1f",
      mean(SMOG, na.rm = TRUE),
      sd(SMOG, na.rm = TRUE)
    ),
    `Coleman-Liau` = sprintf(
      "%.1f ± %.1f",
      mean(`Coleman.Liau.short`, na.rm = TRUE),
      sd(`Coleman.Liau.short`, na.rm = TRUE)
    ),
    ARI = sprintf("%.1f ± %.1f", mean(ARI, na.rm = TRUE), sd(ARI, na.rm = TRUE))
  )

readability_per_model_gt <- read_by_model %>%
  mutate(
    model = tools::toTitleCase(model)
  ) %>%
  gt() %>%
  tab_header(title = "Table 1: Readability Metrics by Model") %>%
  fmt_missing(missing_text = "-")

readability_per_model_gt
```

```{r}
#| label: density-plot-flesch-by-model

ggplot(readability_scores, aes(x = Flesch.Kincaid, fill = model)) +
  geom_density(alpha = 0.4) +
  labs(
    title = "Distribution of Flesch Reading Ease by Model",
    x = "Flesch Reading Ease",
    y = "Density"
  ) +
  scale_fill_manual(values = ai_colors)
```

```{r}
#| label: violin-plot-flesch-model
#| message: false
#| warning: false

p <- readability_scores %>%
  ggplot(
    aes(x = model, y = Flesch.Kincaid, fill = model)
  ) +
  geom_violin(trim = FALSE, alpha = 0.6) +
  geom_jitter(width = 0.2, size = 1.5, alpha = 0.7) +
  stat_summary(
    fun = mean,
    geom = "point",
    shape = 23,
    size = 3,
    fill = "white"
  ) +
  geom_hline(
    yintercept = mean(readability_scores$Flesch.Kincaid, na.rm = TRUE),
    linetype = "dashed",
    color = "red",
    size = 0.8
  ) +
  annotate(
    "text",
    x = 5.5,
    y = mean(readability_scores$Flesch.Kincaid, na.rm = TRUE) + 10,
    label = sprintf(
      "Mean: %.2f",
      mean(readability_scores$Flesch.Kincaid, na.rm = TRUE)
    ),
    color = "red",
    size = 4,
    hjust = 0
  ) +
  labs(
    title = "Flesch Reading Ease by Model",
    subtitle = "Violin + individual scores + mean",
    x = NULL,
    y = "Flesch Reading Ease"
  ) +
  scale_fill_manual(values = ai_colors) +
  coord_flip() +
  theme_minimal(base_size = 16) +
  theme(
    # axis.text.x      = element_text(angle = 45, hjust = 1),
    # panel.grid.minor = element_blank(),
    legend.position = "none"
  )

p

ggsave(
  "figures/flesch_model_violin_plot.svg",
  p,
  width = 7,
  height = 5
)

ggsave(
  "figures/flesch_model_violin_plot.pdf",
  p,
  width = 7,
  height = 5
)
```

```{r}
#| label: barplot-flesch-model

# Create the data frame
readability_data <- data.frame(
  model = c("Anthropic", "Deepseek", "Google", "Openai", "Perplexity", "Xai"),
  flesch_kg_mean = c(35.2, 14.6, 11.9, 15.0, 21.1, 14.4),
  flesch_kg_sd = c(20.5, 3.0, 1.8, 3.1, 9.9, 2.6)
)

# Define color mapping with correct capitalization
ai_colors_capital <- c(
  "Openai" = "#17A683",
  "Google" = "#528DD5",
  "Deepseek" = "#5370FE",
  "Perplexity" = "#278491",
  "Anthropic" = "#D5A583",
  "Xai" = "#000000"
)

# Make model a factor to preserve order
readability_data$model <- factor(
  readability_data$model,
  levels = names(ai_colors_capital)
)

# Create the plot
p <- ggplot(
  readability_data,
  aes(x = reorder(model, -flesch_kg_mean), y = flesch_kg_mean, fill = model)
) +
  geom_bar(stat = "identity", color = NA) +
  geom_errorbar(
    aes(
      ymin = flesch_kg_mean - flesch_kg_sd,
      ymax = flesch_kg_mean + flesch_kg_sd
    ),
    width = 0.2
  ) +
  scale_fill_manual(values = ai_colors_capital) +
  labs(
    title = "Flesch Kincaid Grade by Model",
    y = "Flesch Kincaid Grade (mean ± SD)",
    x = "Model"
  ) +
  theme(
    legend.position = "none",
    axis.title.x = element_blank()
  )

ggsave(
  "figures/flesch_model_bar_plot.pdf",
  p,
  width = 7,
  height = 5
)
```


```{r}
#| label: Readability-per-category-and-model

read_by_category_model <- readability_scores %>%
  group_by(category, model) %>%
  summarise(
    n_responses = n(),
    `Flesch Ease` = sprintf(
      "%.1f ± %.1f",
      mean(Flesch, na.rm = TRUE),
      sd(Flesch, na.rm = TRUE)
    ),
    `Flesch K-G` = sprintf(
      "%.1f ± %.1f",
      mean(Flesch.Kincaid, na.rm = TRUE),
      sd(Flesch.Kincaid, na.rm = TRUE)
    ),
    `Gunning Fog` = sprintf(
      "%.1f ± %.1f",
      mean(FOG, na.rm = TRUE),
      sd(FOG, na.rm = TRUE)
    ),
    SMOG = sprintf(
      "%.1f ± %.1f",
      mean(SMOG, na.rm = TRUE),
      sd(SMOG, na.rm = TRUE)
    ),
    `Coleman-Liau` = sprintf(
      "%.1f ± %.1f",
      mean(`Coleman.Liau.short`, na.rm = TRUE),
      sd(`Coleman.Liau.short`, na.rm = TRUE)
    ),
    ARI = sprintf("%.1f ± %.1f", mean(ARI, na.rm = TRUE), sd(ARI, na.rm = TRUE))
  )

read_by_category_model %>%
  mutate(
    model = tools::toTitleCase(model)
  ) %>%
  gt() %>%
  tab_header(title = "Table 2: Readability Metrics by Category and Model") %>%
  fmt_missing(missing_text = "-")
```



```{r}
#| label: density-plot-flesch-by-model-category

ggplot(readability_scores, aes(x = Flesch, fill = model)) +
  geom_density(alpha = 0.5) +
  facet_wrap(~category) +
  labs(
    title = "Flesch Reading Ease across Models by Category",
    x = "Flesch Reading Ease",
    y = "Density"
  ) +
  scale_fill_manual(values = ai_colors)

```

```{r}
#| label: violin-plot-flesch-model-category
#| message: false
#| warning: false

p <- ggplot(
  readability_scores,
  aes(x = model, y = Flesch.Kincaid, fill = model)
) +
  geom_violin(trim = FALSE, alpha = 0.6) +
  geom_jitter(width = 0.2, size = 1.5, alpha = 0.7) +
  stat_summary(
    fun = mean,
    geom = "point",
    shape = 23,
    size = 3,
    fill = "white"
  ) +
  facet_wrap(~category, ncol = 1) +
  labs(
    title = "Flesch Reading Ease by Model and Category",
    subtitle = "Violin + individual scores + mean",
    x = NULL,
    y = "Flesch Reading Ease"
  ) +
  scale_fill_manual(values = ai_colors) +
  theme_minimal(base_size = 14) +
  theme(
    # axis.text.x      = element_text(angle = 45, hjust = 1),
    # panel.grid.minor = element_blank(),
    legend.position = "none"
  )

p
```

#### B) Word Count

```{r}
#| label: wc-stats-calc

# Compute word counts for each unique response, after cleaning markdown
wordcount_scores <- unique_responses %>%
  mutate(
    clean_response = clean_text_for_readability(response),
    WordCount = str_count(clean_response, "\\S+")
  )
```

```{r}
#| label: wc-by-model

wc_by_model <- wordcount_scores %>%
  group_by(model) %>%
  summarise(
    `Response Count` = n(),
    `Word Count` = sprintf(
      "%.1f ± %.1f",
      mean(WordCount, na.rm = TRUE),
      sd(WordCount, na.rm = TRUE)
    )
  )

gt(wc_by_model) %>%
  tab_header(
    title = "Table 3: Response Characteristics by Model (Word Count)"
  ) %>%
  fmt_missing(missing_text = "-")
```

```{r}
#| label: wc-violin-model

ggplot(wordcount_scores, aes(x = model, y = WordCount, fill = model)) +
  geom_violin(trim = FALSE, alpha = 0.6) +
  geom_jitter(width = 0.2, size = 1.2, alpha = 0.6) +
  stat_summary(
    fun = mean,
    geom = "point",
    shape = 23,
    size = 3,
    fill = "white"
  ) +
  labs(
    title = "Word Count by Model",
    subtitle = "Violin + Jittered Points + Mean",
    x = NULL,
    y = "Word Count"
  ) +
  scale_fill_manual(values = ai_colors) +
  theme(
    axis.text.x = element_text(angle = 45, hjust = 1),
    panel.grid.minor = element_blank(),
    legend.position = "none"
  )
```



### 1.2 Overall Performance by Model

```{r}
#| label: calc-overall-performance-by-model

model_performance <- df_long %>%
  group_by(model) %>%
  summarise(
    mean_rating = mean(rating, na.rm = TRUE),
    sd_rating = sd(rating, na.rm = TRUE),
    se_rating = sd_rating / sqrt(n()),
    n_ratings = n()
  )
```

```{r}
#| label: overall-model-performance-plot

p_model_overall <- ggplot(
  model_performance,
  aes(x = reorder(model, -mean_rating), y = mean_rating)
) +
  geom_bar(stat = "identity", aes(fill = model), width = 0.7) +
  scale_fill_manual(values = ai_colors) +
  geom_errorbar(
    aes(ymin = mean_rating - se_rating, ymax = mean_rating + se_rating),
    width = 0.3,
    color = brand_colors$alert_red
  ) +
  geom_text(
    aes(label = sprintf("%.2f", mean_rating)),
    vjust = -0.5,
    size = 4,
    color = brand_colors$primary_navy
  ) +
  coord_cartesian(ylim = c(0, 5)) +
  labs(
    title = "Mean rating ± SE across all raters and features",
    subtitle = element_blank(),
    x = NULL,
    y = "Mean Rating (1-5 scale)"
  ) +
  scale_x_discrete(labels = function(x) tools::toTitleCase(x)) +
  theme(
    axis.text.x = element_text(angle = 45, hjust = 1),
    legend.position = "none"
  )

print(p_model_overall)

# Save for publication
ggsave(
  "figures/figure1_overall_model_performance.pdf",
  p_model_overall,
  width = 8,
  height = 6
)
```

#### 1.2.1 Model Preferred by Rater

```{r}
#| label: model-preferences-data-from-raters

df_preferences <- read_csv(
  "./data/data_preparation/preferences_20250606_115527.csv",
  show_col_types = FALSE
)

# Calculate preference counts
pref_counts <- df_preferences %>%
  count(preferred_model, name = "count") %>%
  mutate(
    percentage = round(count / sum(count) * 100, 1),
    preferred_model = tools::toTitleCase(preferred_model)
  ) %>%
  arrange(desc(count))

# Overall preference table
pref_counts %>%
  gt() %>%
  cols_label(
    preferred_model = "Preferred Model",
    count = "Count",
    percentage = "Percentage (%)"
  ) %>%
  tab_header(
    title = "Overall Model Preferences Across All Raters"
  )
```

```{r}
#| label: barplot-rater-preferences-overall

p_pref_overall <- ggplot(pref_counts, aes(x = reorder(preferred_model, count), y = count)) +
  geom_bar(stat = "identity", fill = brand_colors$primary_navy) +
  geom_text(aes(label = paste0(count, " (", percentage, "%)")),
    hjust = -0.1, size = 4
  ) +
  coord_flip() +
  labs(
    title = "Model Preferences Across All Raters",
    x = "Model",
    y = "Number of Preferences"
  )

print(p_pref_overall)
```


```{r}
#| label: preferences-by-rater-student-expert

pref_by_rater <- df_preferences %>%
  # Create anonymized rater names
  mutate(
    rater_display = case_when(
      rater_name == "Isabel_LLM" ~ "Student 1",
      rater_name == "Jule_LLM" ~ "Student 2",
      rater_name == "Lasse_LLM" ~ "Student 3",
      rater_name == "Christoph_LLM" ~ "Expert 1",
      rater_name == "Musti_LLM" ~ "Expert 2",
      rater_name == "Charlotte_LLM" ~ "Expert 3",
      # Keep original name for auto-raters
      str_detect(rater_name, "autoLLM_") ~ str_replace(rater_name, "autoLLM_", ""),
      TRUE ~ rater_name
    )
  ) %>%
  count(rater_display, preferred_model) %>%
  group_by(rater_display) %>%
  mutate(
    percentage = round(n / sum(n) * 100, 1),
    preferred_model = tools::toTitleCase(preferred_model)
  ) %>%
  arrange(rater_display, desc(n))

pref_by_rater %>%
  gt(groupname_col = "rater_display") %>%
  cols_label(
    preferred_model = "Preferred Model",
    n = "Count",
    percentage = "Percentage (%)"
  ) %>%
  tab_header(
    title = "Model Preferences by Individual Rater"
  )
```

```{r}
# Preferences by rater type (Expert vs Student)
pref_by_rater_type <- df_preferences %>%
  filter(grader_type == "human") %>% # Only human raters
  mutate(
    rater_type = case_when(
      rater_name %in% c("Christoph_LLM", "Musti_LLM", "Charlotte_LLM") ~ "Expert",
      rater_name %in% c("Isabel_LLM", "Jule_LLM", "Lasse_LLM") ~ "Student",
      TRUE ~ "Other"
    )
  ) %>%
  filter(rater_type != "Other") %>%
  count(rater_type, preferred_model) %>%
  group_by(rater_type) %>%
  mutate(
    percentage = round(n / sum(n) * 100, 1),
    preferred_model = tools::toTitleCase(preferred_model)
  )

# Dodged bar plot comparing Expert vs Student preferences
p_pref_expert_student <- ggplot(
  pref_by_rater_type,
  aes(x = preferred_model, y = percentage, fill = rater_type)
) +
  geom_bar(stat = "identity", position = "dodge", width = 0.7) +
  geom_text(aes(label = paste0(percentage, "%")),
    position = position_dodge(width = 0.7),
    vjust = -0.3, size = 3.5
  ) +
  scale_fill_manual(
    values = c(
      "Expert" = brand_colors$primary_navy,
      "Student" = brand_colors$supporting_blue
    )
  ) +
  labs(
    title = "Model Preferences: Experts vs Students",
    x = "Preferred Model",
    y = "Percentage (%)",
    fill = "Rater Type"
  ) +
  theme_minimal(base_size = 14) +
  theme(
    axis.text.x = element_text(angle = 45, hjust = 1),
    legend.position = "top"
  )

print(p_pref_expert_student)
```

```{r}
#| label: barplot-preferences-expert-student-facetwrap-category

pref_by_rater_type_category <- df_preferences %>%
  filter(grader_type == "human") %>% # Only human raters
  mutate(
    rater_type = case_when(
      rater_name %in% c("Christoph_LLM", "Musti_LLM", "Charlotte_LLM") ~ "Expert",
      rater_name %in% c("Isabel_LLM", "Jule_LLM", "Lasse_LLM") ~ "Student",
      TRUE ~ "Other"
    )
  ) %>%
  filter(rater_type != "Other") %>%
  count(category, rater_type, preferred_model) %>%
  group_by(category, rater_type) %>%
  mutate(
    percentage = round(n / sum(n) * 100, 1),
    preferred_model = tools::toTitleCase(preferred_model)
  ) %>%
  # show missing data explicitly in plot
  ungroup() %>%
  complete(category, rater_type, preferred_model, fill = list(n = 0, percentage = 0))

# Faceted dodged bar plot by category
p_pref_expert_student_category <- ggplot(
  pref_by_rater_type_category,
  aes(x = preferred_model, y = percentage, fill = rater_type)
) +
  geom_bar(stat = "identity", position = "dodge", width = 0.7) +
  geom_text(aes(label = paste0(percentage, "%")),
    position = position_dodge(width = 0.7),
    vjust = -0.3, size = 3
  ) +
  scale_fill_manual(
    values = c(
      "Expert" = brand_colors$primary_navy,
      "Student" = brand_colors$supporting_blue
    )
  ) +
  facet_wrap(~category, scales = "free_y", nrow = 3) +
  labs(
    title = "Model Preferences: Experts vs Students by Category",
    x = "Preferred Model",
    y = "Percentage (%)",
    fill = "Rater Type"
  ) +
  theme_minimal(base_size = 12) +
  theme(
    axis.text.x = element_text(angle = 45, hjust = 1),
    legend.position = "top",
    strip.text = element_text(size = 10, face = "bold")
  )

print(p_pref_expert_student_category)

# Save the plot
ggsave("figures/model_preferences_expert_vs_student_by_category.pdf",
  p_pref_expert_student_category,
  width = 8, height = 10
)
```

```{r}
#| label: table-preferences-by-rater-type-category

pref_by_rater_type_category %>%
  select(category, rater_type, preferred_model, n, percentage) %>%
  arrange(category, rater_type, desc(percentage)) %>%
  gt(groupname_col = c("category", "rater_type")) %>%
  cols_label(
    rater_type = "Rater Type",
    preferred_model = "Preferred Model",
    n = "Count",
    percentage = "Percentage (%)"
  ) %>%
  tab_header(
    title = "Model Preferences by Rater Type and Category"
  ) %>%
  fmt_number(columns = percentage, decimals = 1)
```

```{r}
#| label: stats-significance-tests-model-preferences

# Chi-square test for overall preferences
pref_chisq <- chisq.test(pref_counts$count)

# Create contingency table for preferences by rater type
pref_contingency <- df_preferences %>%
  count(grader_type, preferred_model) %>%
  pivot_wider(names_from = preferred_model, values_from = n, values_fill = 0) %>%
  column_to_rownames("grader_type") %>%
  as.matrix()

# Check contingency table dimensions and structure
cat("Contingency table dimensions:", dim(pref_contingency), "\n")
print(pref_contingency)

# Chi-square test for preferences by rater type
pref_rater_chisq <- chisq.test(pref_contingency)

# Fisher's exact test (only if we have at least 2x2 table)
if (nrow(pref_contingency) >= 2 && ncol(pref_contingency) >= 2) {
  pref_fisher <- fisher.test(pref_contingency, simulate.p.value = TRUE)
  fisher_p <- pref_fisher$p.value
} else {
  cat("Warning: Fisher's exact test requires at least 2x2 table. Skipping.\n")
  fisher_p <- NA
}

# Binomial test for most preferred model
most_preferred <- pref_counts$preferred_model[1]
most_preferred_count <- pref_counts$count[1]
total_preferences <- sum(pref_counts$count)

# Test if most preferred model is significantly different from equal preference
equal_prob <- 1 / nrow(pref_counts)
binom_test <- binom.test(
  most_preferred_count,
  total_preferences,
  p = equal_prob,
  alternative = "greater"
)
```

```{r}
# Create significance summary table (conditionally include Fisher's test)
if (!is.na(fisher_p)) {
  sig_tests <- data.frame(
    Test = c(
      "Overall preference distribution",
      "Preferences differ by rater type",
      paste(most_preferred, "vs. equal preference"),
      "Rater type independence (Fisher's exact)"
    ),
    Statistic = c(
      sprintf("χ² = %.2f", pref_chisq$statistic),
      sprintf("χ² = %.2f", pref_rater_chisq$statistic),
      sprintf("Binomial p = %.4f", binom_test$p.value),
      sprintf("Fisher's p = %.4f", fisher_p)
    ),
    `P-value` = c(
      ifelse(pref_chisq$p.value < 0.001, "<0.001", sprintf("%.4f", pref_chisq$p.value)),
      ifelse(pref_rater_chisq$p.value < 0.001, "<0.001", sprintf("%.4f", pref_rater_chisq$p.value)),
      ifelse(binom_test$p.value < 0.001, "<0.001", sprintf("%.4f", binom_test$p.value)),
      ifelse(fisher_p < 0.001, "<0.001", sprintf("%.4f", fisher_p))
    ),
    Interpretation = c(
      ifelse(pref_chisq$p.value < 0.05, "Significant", "Not significant"),
      ifelse(pref_rater_chisq$p.value < 0.05, "Significant", "Not significant"),
      ifelse(binom_test$p.value < 0.05, "Significantly preferred", "Not significantly preferred"),
      ifelse(fisher_p < 0.05, "Preferences depend on rater type", "Preferences independent of rater type")
    )
  )
} else {
  sig_tests <- data.frame(
    Test = c(
      "Overall preference distribution",
      "Preferences differ by rater type",
      paste(most_preferred, "vs. equal preference")
    ),
    Statistic = c(
      sprintf("χ² = %.2f", pref_chisq$statistic),
      sprintf("χ² = %.2f", pref_rater_chisq$statistic),
      sprintf("Binomial p = %.4f", binom_test$p.value)
    ),
    `P-value` = c(
      ifelse(pref_chisq$p.value < 0.001, "<0.001", sprintf("%.4f", pref_chisq$p.value)),
      ifelse(pref_rater_chisq$p.value < 0.001, "<0.001", sprintf("%.4f", pref_rater_chisq$p.value)),
      ifelse(binom_test$p.value < 0.001, "<0.001", sprintf("%.4f", binom_test$p.value))
    ),
    Interpretation = c(
      ifelse(pref_chisq$p.value < 0.05, "Significant", "Not significant"),
      ifelse(pref_rater_chisq$p.value < 0.05, "Significant", "Not significant"),
      ifelse(binom_test$p.value < 0.05, "Significantly preferred", "Not significantly preferred")
    )
  )
}
```

```{r}
# Display significance test results
sig_tests %>%
  gt() %>%
  cols_label(
    Test = "Statistical Test",
    Statistic = "Test Statistic",
    `P.value` = "P-value",
    Interpretation = "Interpretation"
  ) %>%
  tab_header(
    title = "Statistical Significance Tests for Model Preferences"
  ) %>%
  tab_footnote(
    footnote = "Tests whether observed preferences differ significantly from chance",
    locations = cells_title()
  )
```

### 1.3 Overall Performance by Feature

```{r overall-feature-performance}
# Calculate overall performance metrics by feature
feature_performance <- df_long %>%
  group_by(feature) %>%
  summarise(
    mean_rating = mean(rating, na.rm = TRUE),
    sd_rating = sd(rating, na.rm = TRUE),
    se_rating = sd_rating / sqrt(n()),
    n_ratings = n()
  )

# Create horizontal bar chart
p_feature_overall <- ggplot(
  feature_performance,
  aes(x = reorder(feature, mean_rating), y = mean_rating)
) +
  geom_bar(
    stat = "identity",
    fill = brand_colors$supporting_blue,
    width = 0.7
  ) +
  geom_errorbar(
    aes(ymin = mean_rating - se_rating, ymax = mean_rating + se_rating),
    width = 0.3,
    color = brand_colors$alert_red
  ) +
  geom_text(
    aes(label = sprintf("%.2f", mean_rating)),
    hjust = -0.2,
    size = 4,
    color = brand_colors$primary_navy
  ) +
  coord_flip(xlim = c(0.5, 9.5), ylim = c(0, 5)) +
  labs(
    title = "Mean rating ± SE across all models and raters",
    x = "Feature",
    y = "Mean Rating (1-5 scale)"
  ) +
  theme(axis.title.y = element_blank())

print(p_feature_overall)

# Save for publication
ggsave(
  "figures/figure2_overall_feature_performance.pdf",
  p_feature_overall,
  width = 8,
  height = 6
)
```

### 1.4 Per-Rater Summary Statistics

```{r rater-summary}
# Calculate per-rater statistics
rater_summary <- df_long %>%
  group_by(rater_name, rater_type_detailed) %>%
  summarise(
    mean_rating = mean(rating, na.rm = TRUE),
    sd_rating = sd(rating, na.rm = TRUE),
    median_rating = median(rating, na.rm = TRUE),
    n_ratings = n(),
    .groups = "drop"
  ) %>%
  arrange(rater_type_detailed, desc(mean_rating)) |>
  # pseudonymize human raters
  group_by(rater_type_detailed) %>% # group temporarily for row_number
  mutate(
    rater_display = case_when(
      rater_type_detailed == "Expert" ~ paste("Expert", row_number()),
      rater_type_detailed == "Student" ~ paste("Student", row_number()),
      rater_type_detailed == "Auto-grader" ~
        case_when(
          str_detect(rater_name, "openai_4o_strict") ~
            "OpenAI GPT-4o (Stricter)",
          str_detect(rater_name, "openai_4o") ~ "OpenAI GPT-4o",
          str_detect(rater_name, "gemini_2.5_pro") ~ "Gemini 2.5 Pro",
          str_detect(rater_name, "gemini_2.0_flash") ~ "Gemini 2.0 Flash",
          str_detect(rater_name, "anthropic_somnet4") ~ "Anthropic Sonnet 4",
          str_detect(rater_name, "anthropic_somnet3.5") ~
            "Anthropic Sonnet 3.5",
          TRUE ~ rater_name
        )
    )
  ) %>%
  ungroup()

# Create visualization
p_rater_summary <- ggplot(
  rater_summary,
  aes(
    x = reorder(rater_display, mean_rating),
    y = mean_rating,
    fill = rater_type_detailed
  )
) +
  geom_bar(stat = "identity", width = 0.7) +
  geom_errorbar(
    aes(
      ymin = mean_rating - sd_rating / sqrt(n_ratings),
      ymax = mean_rating + sd_rating / sqrt(n_ratings)
    ),
    width = 0.3
  ) +
  geom_hline(
    yintercept = mean(rater_summary$mean_rating),
    linetype = "dashed",
    color = brand_colors$alert_red
  ) +
  scale_fill_manual(
    values = c(
      "Student" = brand_colors$supporting_blue,
      "Expert" = brand_colors$primary_navy,
      "Auto-grader" = brand_colors$accent_yellow
    )
  ) +
  coord_flip() +
  labs(
    title = "Individual Rater Tendencies",
    subtitle = "Mean rating ± SE across all features and models",
    x = "Rater",
    y = "Mean Rating (1-5 scale)",
    fill = "Rater Type"
  )

print(p_rater_summary)

# Create summary table
rater_table <- rater_summary %>%
  mutate(
    Rating = sprintf("%.2f ± %.2f", mean_rating, sd_rating)
  ) %>%
  select(
    `Rater` = rater_display,
    `Type` = rater_type_detailed,
    `Mean ± SD` = Rating,
    `Median` = median_rating,
    `N` = n_ratings
  )

kableExtra::kable(
  rater_table,
  caption = "Table 2. Individual rater summary statistics",
  # format = "latex",
  booktabs = TRUE
) %>%
  kable_styling(latex_options = c("striped", "hold_position"))
```

## 2. Feature-Specific Performance Analysis

### 2.1 Model Performance by Feature

```{r model-feature-heatmap}
# Calculate mean ratings for model-feature combinations
model_feature_matrix <- df_long %>%
  group_by(model, feature) %>%
  summarise(mean_rating = mean(rating, na.rm = TRUE), .groups = "drop") %>%
  pivot_wider(names_from = feature, values_from = mean_rating) %>%
  column_to_rownames("model") %>%
  as.matrix()

# Create heatmap
p_heatmap <- model_feature_matrix %>%
  as.data.frame() %>%
  rownames_to_column("model") %>%
  pivot_longer(-model, names_to = "feature", values_to = "rating") %>%
  mutate(
    model = factor(
      model,
      levels = names(ai_colors),
      labels = tools::toTitleCase(names(ai_colors))
    ),
    feature = factor(feature, levels = rating_features)
  ) %>%
  ggplot(aes(x = feature, y = model, fill = rating)) +
  geom_tile(color = "white", linewidth = 0.5) +
  geom_text(aes(label = sprintf("%.2f", rating)), size = 6) +
  scale_fill_gradient2(
    low = brand_colors$alert_red,
    mid = brand_colors$accent_yellow,
    high = brand_colors$success_green,
    midpoint = 3,
    limits = c(1, 5),
    name = "Rating"
  ) +
  labs(
    title = "Mean ratings averaged across all raters",
    x = "Feature",
    y = "Model"
  ) +
  theme(
    axis.title.x = element_blank(),
    axis.title.y = element_blank(),
    axis.text.x = element_text(angle = 45, hjust = 1, size = 14),
    axis.text.y = element_text(size = 14),
    legend.position = "right"
  )

print(p_heatmap)

# Save for publication
ggsave(
  "figures/figure3_model_feature_heatmap.pdf",
  p_heatmap,
  width = 10,
  height = 8
)
```

```{r}
#| label: feature-stats

# 1. Numeric summaries
feature_stats <- df_long %>%
  group_by(model, feature) %>%
  summarise(
    mean_rating = mean(rating),
    sd_rating = sd(rating),
    n = n(),
    se = sd_rating / sqrt(n),
    ci_lower = mean_rating - qt(0.975, n - 1) * se,
    ci_upper = mean_rating + qt(0.975, n - 1) * se,
    .groups = "drop"
  )

# 2. Pivot for display: combine mean & CI
feature_wide <- feature_stats %>%
  mutate(
    Mean_CI = sprintf("%0.2f (%0.2f–%0.2f)", mean_rating, ci_lower, ci_upper)
  ) %>%
  select(feature, model, Mean_CI) %>%
  pivot_wider(
    names_from = model,
    values_from = Mean_CI
  )

# 3. Identify highest and lowest feature for each model
best_worst <- feature_stats %>%
  group_by(model) %>%
  summarise(
    Highest = feature[which.max(mean_rating)],
    Highest_val = max(mean_rating),
    Lowest = feature[which.min(mean_rating)],
    Lowest_val = min(mean_rating),
    .groups = "drop"
  )
```

```{r}
#| label: feature-table-wide
#| ft.keepnext: false

# Create a landscape table for wide content
if (knitr::is_latex_output()) {
  # Use LaTeX environment instead of direct commands
  cat("\\begin{landscape}\n")
}

feature_wide %>%
  gt(rowname_col = "feature") %>%
  cols_label(
    feature = "Feature",
    anthropic = "Anthropic",
    deepseek = "DeepSeek",
    google = "Google",
    openai = "OpenAI",
    perplexity = "Perplexity",
    xai = "XAI"
  ) %>%
  tab_header(
    title = "Mean Feature Ratings (95% CI) by Model"
  ) %>%
  tab_options(
    table.width = pct(100),
    table.font.size = px(9) # Reduce font size for PDF
  )

if (knitr::is_latex_output()) {
  cat("\\end{landscape}\n")
}
```

```{r}
#| label: best-worst-ci

# calculated feature_stats and best_worst before:
# feature_stats has columns: model, feature, mean_rating, ci_lower, ci_upper
# best_worst has: model, Highest, Highest_val, Lowest, Lowest_val

best_worst_ci <- best_worst %>%
  left_join(
    feature_stats %>% select(model, feature, ci_lower, ci_upper),
    by = c("model", "Highest" = "feature")
  ) %>%
  rename(
    high_ci_lower = ci_lower,
    high_ci_upper = ci_upper
  ) %>%
  left_join(
    feature_stats %>% select(model, feature, ci_lower, ci_upper),
    by = c("model", "Lowest" = "feature")
  ) %>%
  rename(
    low_ci_lower = ci_lower,
    low_ci_upper = ci_upper
  ) %>%
  mutate(
    `Top Feature (Mean [95% CI])` = sprintf(
      "%s (%.2f [%.2f–%.2f])",
      Highest,
      Highest_val,
      high_ci_lower,
      high_ci_upper
    ),
    `Bottom Feature (Mean [95% CI])` = sprintf(
      "%s (%.2f [%.2f–%.2f])",
      Lowest,
      Lowest_val,
      low_ci_lower,
      low_ci_upper
    ),
    # Capitalize model names
    Model = recode(
      model,
      anthropic = "Anthropic",
      deepseek = "DeepSeek",
      google = "Google",
      openai = "OpenAI",
      perplexity = "Perplexity",
      xai = "XAI"
    )
  ) %>%
  select(Model, `Top Feature (Mean [95% CI])`, `Bottom Feature (Mean [95% CI])`)

# Render GT table
best_worst_ci %>%
  gt() %>%
  cols_label(
    Model = "Model",
    `Top Feature (Mean [95% CI])` = "Highest",
    `Bottom Feature (Mean [95% CI])` = "Lowest"
  ) %>%
  tab_header(
    title = "Best- and Worst-Rated Features by Model"
  )

```


#### Interpretation

* **Anthropic**
  
  * Highest: **`r best_worst$Highest[best_worst$model=="anthropic"]`**
  (mean = `r sprintf("%.2f", best_worst$Highest_val[best_worst$model=="anthropic"])`).
  * Lowest: **`r best_worst$Lowest[best_worst$model=="anthropic"]`**
  (mean = `r sprintf("%.2f", best_worst$Lowest_val[best_worst$model=="anthropic"])`).

* **DeepSeek**
  
  * Highest: **`r best_worst$Highest[best_worst$model=="deepseek"]`**
  (mean = `r sprintf("%.2f", best_worst$Highest_val[best_worst$model=="deepseek"])`).
  * Lowest: **`r best_worst$Lowest[best_worst$model=="deepseek"]`**
  (mean = `r sprintf("%.2f", best_worst$Lowest_val[best_worst$model=="deepseek"])`).

* **Google**
  
  * Highest: **`r best_worst$Highest[best_worst$model=="google"]`**
  (mean = `r sprintf("%.2f", best_worst$Highest_val[best_worst$model=="google"])`).
  * Lowest: **`r best_worst$Lowest[best_worst$model=="google"]`**
  (mean = `r sprintf("%.2f", best_worst$Lowest_val[best_worst$model=="google"])`).

* **OpenAI**
  
  * Highest: **`r best_worst$Highest[best_worst$model=="openai"]`**
  (mean = `r sprintf("%.2f", best_worst$Highest_val[best_worst$model=="openai"])`).
  * Lowest: **`r best_worst$Lowest[best_worst$model=="openai"]`**
  (mean = `r sprintf("%.2f", best_worst$Lowest_val[best_worst$model=="openai"])`).

* **Perplexity**
  
  * Highest: **`r best_worst$Highest[best_worst$model=="perplexity"]`**
  (mean = `r sprintf("%.2f", best_worst$Highest_val[best_worst$model=="perplexity"])`).
  * Lowest: **`r best_worst$Lowest[best_worst$model=="perplexity"]`**
  (mean = `r sprintf("%.2f", best_worst$Lowest_val[best_worst$model=="perplexity"])`).

* **XAI**
  
  * Highest: **`r best_worst$Highest[best_worst$model=="xai"]`**
  (mean = `r sprintf("%.2f", best_worst$Highest_val[best_worst$model=="xai"])`).
  * Lowest: **`r best_worst$Lowest[best_worst$model=="xai"]`**
  (mean = `r sprintf("%.2f", best_worst$Lowest_val[best_worst$model=="xai"])`).

> Clinicians can use these highlights—each model’s strongest and weakest feature—to decide which system best fits their specific priorities.




### 2.2 Rater Type Comparison by Feature

```{r rater-type-feature}
# Calculate mean ratings by rater type and feature
rater_type_feature <- df_long %>%
  group_by(rater_type_detailed, feature) %>%
  summarise(
    mean_rating = mean(rating, na.rm = TRUE),
    se_rating = sd(rating, na.rm = TRUE) / sqrt(n()),
    .groups = "drop"
  )

# Create grouped bar chart
p_rater_type_feature <- ggplot(
  rater_type_feature,
  aes(x = feature, y = mean_rating, fill = rater_type_detailed)
) +
  geom_bar(
    stat = "identity",
    position = position_dodge(width = 0.8),
    width = 0.7
  ) +
  geom_errorbar(
    aes(ymin = mean_rating - se_rating, ymax = mean_rating + se_rating),
    position = position_dodge(width = 0.8),
    width = 0.3
  ) +
  scale_fill_manual(
    values = c(
      "Student" = brand_colors$supporting_blue,
      "Expert" = brand_colors$primary_navy,
      "Auto-grader" = brand_colors$accent_yellow
    )
  ) +
  labs(
    title = "Feature Ratings by Rater Type",
    subtitle = "Comparison of rating patterns across different evaluator groups",
    x = "Feature",
    y = "Mean Rating (1-5 scale)",
    fill = "Rater Type"
  ) +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))

p_rater_type_feature

ggsave(
  "figures/figure4_mean_ratings_ratergroup_feature.pdf",
  p_rater_type_feature,
  width = 10,
  height = 8
)
```

## 3. Inter-Rater Reliability Analysis

### 3.1 Within-Group Reliability

We compute the single‐measure, absolute‐agreement ICC (ICC(A,1)) for each rater group (Students, Experts, Auto-graders) using a two-way random‐effects model. First we show ICC for all rating features combined: 


```{r within-group-icc}
#-- Function to compute ICC for a given rater group ------------------------
calculate_group_icc <- function(df_long, group_name) {
  df_long %>%
    filter(rater_type_detailed == group_name) %>%
    # make a unique "subject" ID so that each row in the wide matrix is one
    # (question × model × feature) combination
    mutate(subject = paste(index, model, feature, sep = "_")) %>%
    select(subject, rater_name, rating) %>%
    # pivot to wide: one row per subject, one column per rater
    pivot_wider(
      names_from = rater_name,
      values_from = rating
    ) -> wide_df

  # drop the subject column and coerce to a numeric matrix
  rating_mat <- wide_df %>%
    select(-subject) %>%
    as.matrix()

  # compute ICC: two‐way random, absolute agreement, single‐rater
  irr::icc(rating_mat, model = "twoway", type = "agreement", unit = "single")
}

#-- Compute for each group --------------------------------------------------
icc_students <- calculate_group_icc(df_long, "Student")
icc_experts <- calculate_group_icc(df_long, "Expert")
icc_autograders <- calculate_group_icc(df_long, "Auto-grader")

#-- View the results --------------------------------------------------------
print(icc_students)
print(icc_experts)
print(icc_autograders)
```

```{r}
#| label: icc-within-group-table-overall-features

# build a summary tibble with combined CI column
icc_summary <- tibble(
  `Rater Group` = c("Students", "Experts", "Auto-graders"),
  ICC = c(
    icc_students$value,
    icc_experts$value,
    icc_autograders$value
  ),
  LowerCI = c(
    icc_students$lbound,
    icc_experts$lbound,
    icc_autograders$lbound
  ),
  UpperCI = c(
    icc_students$ubound,
    icc_experts$ubound,
    icc_autograders$ubound
  )
) %>%
  mutate(
    `95% CI` = sprintf("(%0.3f–%0.3f)", LowerCI, UpperCI),
    Interpretation = case_when(
      ICC < 0.50 ~ "Poor agreement",
      ICC < 0.75 ~ "Moderate agreement",
      ICC < 0.90 ~ "Good agreement",
      TRUE ~ "Excellent agreement"
    )
  ) %>%
  select(`Rater Group`, ICC, `95% CI`, Interpretation)

# render as a gt table
icc_summary %>%
  gt() %>%
  fmt_number(
    columns = "ICC",
    decimals = 3
  ) %>%
  cols_label(
    ICC = "ICC(A,1)",
    `95% CI` = "(95% CI)",
    Interpretation = "Interpretation"
  ) %>%
  tab_header(
    title = "Within‐Group ICC(A,1) Results"
  )
```



### 3.1. Within-Group Reliability (Cont'd for each feature category)

```{r}
# Function to compute ICC for each feature category
icc_by_feature <- function(df, group_name, feat) {
  mat <- df %>%
    filter(rater_type_detailed == group_name, feature == feat) %>%
    mutate(subject = paste(index, model, sep = "_")) %>%
    select(subject, rater_name, rating) %>%
    pivot_wider(names_from = rater_name, values_from = rating) %>%
    select(-subject) %>%
    as.matrix()

  out <- irr::icc(mat, model = "twoway", type = "agreement", unit = "single")

  tibble(
    Group = group_name,
    Feature = feat,
    ICC = out$value,
    LowerCI = out$lbound,
    UpperCI = out$ubound
  )
}

# Define groups and features
groups <- c("Student", "Expert", "Auto-grader")
features <- unique(df_long$feature)

# Compute ICC for each combination
icc_feature_all <- map_dfr(groups, function(g) {
  map_dfr(features, function(f) icc_by_feature(df_long, g, f))
})

```


```{r}
#| label: icc-feature-table

icc_feature_all %>%
  arrange(Group, Feature) %>%
  # create a combined CI column and drop the separate bounds
  mutate(
    `95% CI` = sprintf("(%0.3f–%0.3f)", LowerCI, UpperCI)
  ) %>%
  select(Group, Feature, ICC, `95% CI`) %>%
  gt(
    rowname_col = "Feature",
    groupname_col = "Group"
  ) %>%
  fmt_number(
    columns = "ICC",
    decimals = 3
  ) %>%
  cols_label(
    ICC = "ICC(A,1)",
    `95% CI` = "(95% CI)"
  ) %>%
  tab_header(
    title = "Feature-Wise ICC(A,1) by Rater Group"
  )

```


#### Interpretation

**coming soon**

### 3.2 Between-Group Reliability

We’ll use **ICC(2,k)** (two‐way random, absolute‐agreement, *average*‐measure) to ask how reliable the *mean* rating is when we pool different sets of raters:

1. **All 12 raters** (3 Students + 3 Experts + 6 Auto-graders)  
2. **Students + Experts** (6 human raters)  
3. **Experts + Auto-graders** (9 raters)

```{r}
#| label: between-setup

# helper: compute ICC(2,k) for feature `feat` and rater types `groups`
compute_icc_avg <- function(df, feat, groups) {
  df %>%
    filter(feature == feat, rater_type_detailed %in% groups) %>%
    mutate(subject = paste(index, model, sep = "_")) %>%
    select(subject, rater_name, rating) %>%
    pivot_wider(names_from = rater_name, values_from = rating) -> wide

  mat <- wide %>%
    select(-subject) %>%
    as.matrix()
  irr::icc(mat, model = "twoway", type = "agreement", unit = "average")
}

features <- unique(df_long$feature)

between_icc <- map_dfr(features, function(feat) {
  all12 <- compute_icc_avg(df_long, feat, c("Student", "Expert", "Auto-grader"))
  se <- compute_icc_avg(df_long, feat, c("Student", "Expert"))
  ea <- compute_icc_avg(df_long, feat, c("Expert", "Auto-grader"))
  tibble(
    Feature = feat,
    All12 = all12$value,
    All12_Lower = all12$lbound,
    All12_Upper = all12$ubound,
    Students_Experts = se$value,
    SE_Lower = se$lbound,
    SE_Upper = se$ubound,
    Experts_Auto = ea$value,
    EA_Lower = ea$lbound,
    EA_Upper = ea$ubound
  )
})

# combine into human‐readable strings
between_icc <- between_icc %>%
  mutate(
    `All 12 Raters` = sprintf(
      "%0.3f (%0.3f–%0.3f)",
      All12,
      All12_Lower,
      All12_Upper
    ),
    `Students + Experts` = sprintf(
      "%0.3f (%0.3f–%0.3f)",
      Students_Experts,
      SE_Lower,
      SE_Upper
    ),
    `Experts + Auto` = sprintf(
      "%0.3f (%0.3f–%0.3f)",
      Experts_Auto,
      EA_Lower,
      EA_Upper
    )
  ) %>%
  select(Feature, `All 12 Raters`, `Students + Experts`, `Experts + Auto`)
```

```{r}
#| label: between-table

between_icc %>%
  gt(rowname_col = "Feature") %>%
  cols_label(
    `All 12 Raters` = "All 12 Raters",
    `Students + Experts` = "Students + Experts",
    `Experts + Auto` = "Experts + Auto‐graders"
  ) %>%
  tab_header(
    title = "Between‐Group ICC(2,k) by Feature"
  )
```


```{r}
#| label: plot-betweengrp-styled

features <- unique(df_long$feature)
between_icc_num <- map_dfr(features, function(feat) {
  se <- compute_icc_avg(df_long, feat, c("Student", "Expert"))
  ea <- compute_icc_avg(df_long, feat, c("Expert", "Auto-grader"))
  tibble(
    Feature = feat,
    `Students vs Experts` = se$value,
    `Experts vs Auto` = ea$value,
    se_lower = se$lbound,
    se_upper = se$ubound,
    ea_lower = ea$lbound,
    ea_upper = ea$ubound
  )
})

between_long <- between_icc_num %>%
  pivot_longer(
    cols = c(`Students vs Experts`, `Experts vs Auto`),
    names_to = "Comparison",
    values_to = "ICC2k"
  ) %>%
  mutate(
    lower = if_else(Comparison == "Students vs Experts", se_lower, ea_lower),
    upper = if_else(Comparison == "Students vs Experts", se_upper, ea_upper)
  )

# styled plot
ggplot(
  between_long,
  aes(x = Feature, y = ICC2k, color = Comparison, group = Comparison)
) +
  geom_errorbar(
    aes(ymin = lower, ymax = upper),
    width = 0.3,
    size = 0.8,
    position = position_dodge(width = 0.5)
  ) +
  geom_point(size = 4, position = position_dodge(width = 0.5)) +
  geom_hline(
    yintercept = 0.60,
    linetype = "dashed",
    linewidth = 1,
    color = brand_colors$alert_red,
    alpha = 0.7
  ) +
  scale_color_manual(
    values = c(
      "Students vs Experts" = brand_colors$primary_navy,
      "Experts vs Auto" = brand_colors$accent_yellow
    )
  ) +
  labs(
    title = "Between‐Group Reliability Across Features",
    subtitle = "ICC(2,k) for Students vs Experts and Experts vs Auto‐graders",
    x = "Feature",
    y = "ICC(2,k)",
    color = ""
  ) +
  theme_minimal(base_size = 14) +
  theme(
    axis.text.x = element_text(angle = 45, hjust = 1),
    legend.position = "top",
    panel.grid.major.y = element_line(color = "grey80"),
    panel.grid.minor = element_blank()
  ) +
  coord_cartesian(ylim = c(0, 1))

```

#### Comments on Usefulness

* **Average-measure ICC(2,k)** tells you how reliable the *mean* of *k* raters is, which naturally increases as you add more raters (since random error averages out).
* As expected, pooling **all 12** yields the highest reliability for every feature.
* The **Students + Experts** panel (6 human raters) shows noticeably lower ICCs than the full panel, highlighting the value of adding automated scores.
* Combining **Experts + Auto-graders** (9 raters) recovers much of the full-panel reliability, suggesting auto-graders complement expert raters well.

> **Caveat:**  Because our *single-measure* (ICC(2,1)) within-group reliabilities were often only *fair* (≤0.5), these average-measure ICCs should be interpreted with caution.  You're essentially demonstrating that *more* raters produce more reliable *means*—a well-known statistical effect—rather than uncovering a deep alignment between groups.  If you wanted to know *how* well human consensus matches auto-grader consensus *per se*, you'd instead compute ICC(2,1) on the two **group means** (one “human” mean vs. one “auto” mean), but that only makes sense if each group mean is itself reliable (which, here, is marginal for some features).

---

## 4. Model Performance Comparison

### 4.1 Linear Mixed-Effects Models

```{r lmm-analysis}
#| label: lmm-fn-per-feature

fit_feature_lmm <- function(data, feature_name) {
  feature_data <- data %>%
    filter(feature == feature_name)

  # Fit the model with random intercepts for question and rater
  model <- lmer(
    rating ~ model + (1 | index) + (1 | rater_name),
    data = feature_data,
    REML = FALSE
  )

  # Get model summary
  model_summary <- summary(model)

  # Perform likelihood ratio test
  null_model <- lmer(
    rating ~ 1 + (1 | index) + (1 | rater_name),
    data = feature_data,
    REML = FALSE
  )
  lr_test <- anova(null_model, model)

  # Get estimated marginal means
  # emm <- emmeans(model, ~model, lmerTest.limit = 4000, pbkrtest.limit = 4000)  # takes long time
  emm <- suppressMessages(suppressWarnings(emmeans(model, ~model))) # approximate faster
  emm_df <- as.data.frame(emm)

  # Perform pairwise comparisons
  pairs <- pairs(emm, adjust = "tukey")

  return(list(
    feature = feature_name,
    model = model,
    summary = model_summary,
    lr_test = lr_test,
    emmeans = emm_df,
    pairwise = pairs
  ))
}
```

```{r}
#| label: Fit-lmm-all-features

lmm_results <- map(rating_features, ~ fit_feature_lmm(df_long, .x))
names(lmm_results) <- rating_features

# Extract key statistics for each feature
feature_model_stats <- map_df(lmm_results, function(res) {
  lr_p <- res$lr_test$`Pr(>Chisq)`[2]
  data.frame(
    Feature = res$feature,
    LR_Chisq = res$lr_test$Chisq[2],
    df = res$lr_test$Df[2],
    p_value = lr_p,
    Significant = ifelse(
      lr_p < 0.001,
      "***",
      ifelse(lr_p < 0.01, "**", ifelse(lr_p < 0.05, "*", "ns"))
    )
  )
})

# Create estimated marginal means plot for significant features
significant_features <- feature_model_stats %>%
  filter(p_value < 0.05) %>%
  pull(Feature)

if (length(significant_features) > 0) {
  emm_data <- map_df(significant_features, function(feat) {
    lmm_results[[feat]]$emmeans %>%
      mutate(Feature = feat)
  })

  p_emm <- ggplot(
    emm_data,
    aes(x = model, y = emmean, color = model, group = Feature)
  ) +
    geom_errorbar(
      aes(ymin = asymp.LCL, ymax = asymp.UCL),
      width = 0.3,
      size = 0.8,
      position = position_dodge(width = 0.5)
    ) +
    geom_point(size = 3, position = position_dodge(width = 0.5)) +
    facet_wrap(~Feature, scales = "free_x", ncol = 3) +
    scale_color_manual(values = ai_colors) +
    labs(
      title = "Estimated Marginal Means by Model",
      subtitle = "Features with significant model effects (p < 0.05)",
      x = NULL,
      y = "Estimated Mean Rating",
      color = "Model"
    ) +
    theme_minimal(base_size = 14) +
    theme(
      axis.text.x = element_blank(),
      axis.ticks.x = element_blank(),
      legend.position = "bottom",
      panel.grid.major.y = element_line(color = "grey80"),
      panel.grid.minor = element_blank()
    )

  print(p_emm)
}
```

```{r}
#| label: gt-LRT-p-vals-model-effects

feature_model_stats %>%
  gt() %>%
  cols_label(
    Feature = "Feature",
    LR_Chisq = "χ²",
    df = "df",
    p_value = "p-value",
  ) %>%
  fmt_number(
    columns = vars(LR_Chisq),
    decimals = 2
  ) %>%
  fmt_number(
    columns = vars(df),
    decimals = 0
  ) %>%
  fmt_number(
    columns = vars(`p_value`),
    decimals = 4
  ) %>%
  text_transform(
    locations = cells_body(columns = vars(p_value)),
    fn = function(x) {
      ifelse(as.numeric(x) < 0.001, "<0.001", x)
    }
  ) %>%
  tab_header(
    title = "Table 4. Likelihood‐Ratio Tests for Model Effects"
  ) %>%
  opt_row_striping()
```

```{r}
#| label: gt-emm-model-lmm

emm_data %>%
  mutate(
    emmean = round(emmean, 2),
    asymp.LCL = round(asymp.LCL, 2),
    asymp.UCL = round(asymp.UCL, 2)
  ) %>%
  select(
    Feature,
    Model = model,
    `Estimated Mean` = emmean,
    `Lower CI` = asymp.LCL,
    `Upper CI` = asymp.UCL
  ) %>%
  arrange(Feature, Model) %>%
  group_by(Feature) %>%
  gt() %>%
  tab_options(latex.use_longtable = TRUE) %>%
  tab_header(
    title = "Table 5. Estimated Marginal Means for Significant Features"
  ) %>%
  cols_merge(
    columns = c("Estimated Mean", "Lower CI", "Upper CI"),
    pattern = "{1} ({2}-{3})"
  ) %>%
  cols_label(
    `Estimated Mean` = "Estimated Mean (95% CI)"
  ) %>%
  fmt_number(
    columns = c(`Estimated Mean`, `Lower CI`, `Upper CI`),
    decimals = 2
  )
```

### 4.2 Rater Type Effects

```{r rater-type-effects}
#| label: fit-rater-type-lmm

# Function to fit LMM with rater type effects
fit_rater_type_lmm <- function(data, feature_name) {
  feature_data <- data %>%
    filter(feature == feature_name)

  # Fit the full model with interaction
  model_full <- lmer(
    rating ~ model * rater_type_detailed + (1 | index) + (1 | rater_name),
    data = feature_data,
    REML = FALSE
  )

  # Fit reduced model without interaction
  model_reduced <- lmer(
    rating ~ model + rater_type_detailed + (1 | index) + (1 | rater_name),
    data = feature_data,
    REML = FALSE
  )

  # Test interaction effect
  interaction_test <- anova(model_reduced, model_full)

  # Get main effects from reduced model
  emm_rater <- suppressMessages(suppressWarnings(emmeans(
    model_reduced,
    ~rater_type_detailed
  )))
  pairs_rater <- pairs(emm_rater, adjust = "tukey")

  return(list(
    feature = feature_name,
    model_full = model_full,
    model_reduced = model_reduced,
    interaction_p = interaction_test$`Pr(>Chisq)`[2],
    emm_rater = as.data.frame(emm_rater),
    pairs_rater = as.data.frame(pairs_rater)
  ))
}
```

```{r}
#| label: lmm-fit-rater-type-effects

rater_type_results <- map(rating_features, ~ fit_rater_type_lmm(df_long, .x))
names(rater_type_results) <- rating_features
```

```{r rater-type-visualization}
#| label: viz-rater-type-effects

# Extract rater type effects for all features
rater_type_effects <- map_df(rater_type_results, function(res) {
  res$emm_rater %>%
    mutate(
      Feature = res$feature,
      Interaction_p = res$interaction_p
    )
})

# Create visualization
p_rater_effects <- ggplot(
  rater_type_effects,
  aes(x = Feature, y = emmean, color = rater_type_detailed)
) +
  geom_point(size = 3, position = position_dodge(width = 0.3)) +
  geom_errorbar(
    aes(ymin = asymp.LCL, ymax = asymp.UCL),
    width = 0.2,
    position = position_dodge(width = 0.3)
  ) +
  scale_color_manual(
    values = c(
      "Student" = brand_colors$supporting_blue,
      "Expert" = brand_colors$primary_navy,
      "Auto-grader" = brand_colors$accent_yellow
    )
  ) +
  labs(
    title = "Rater Type Effects Across Features",
    subtitle = "Estimated marginal means from mixed-effects models",
    x = "Feature",
    y = "Estimated Mean Rating",
    color = "Rater Type"
  ) +
  theme(axis.text.x = element_text(angle = 45, hjust = 1)) +
  coord_cartesian(ylim = c(3, 5))

print(p_rater_effects)
```

```{r}
#| label: Create-pairwise-comparison-summary

pairwise_summary <- map_df(rater_type_results, function(res) {
  res$pairs_rater %>%
    mutate(
      Feature = res$feature,
      p_adj = p.value,
      Sig = case_when(
        p.value < 0.001 ~ "***",
        p.value < 0.01 ~ "**",
        p.value < 0.05 ~ "*",
        TRUE ~ "ns"
      )
    ) %>%
    select(Feature, contrast, estimate, SE, p_adj, Sig)
})
```

```{r}
#| label: gt-pairwise-comparison-summary-lmm

pairwise_summary %>%
  group_by(Feature) %>%
  gt(rowname_col = "contrast", groupname_col = "Feature") %>%
  fmt_number(
    columns = c(estimate, SE, p_adj),
    decimals = 3
  ) %>%
  cols_label(
    contrast = "Comparison",
    estimate = "Estimate",
    SE = "Std. Error",
    p_adj = "Adjusted p-value",
    Sig = "Significance"
  ) %>%
  tab_header(
    title = "Pairwise Comparisons by Feature"
  ) %>%
  opt_row_striping() %>%
  fmt_missing(
    missing_text = "-"
  )

```


---


## 5. Multivariate Analysis

### 5.1 Principal Component Analysis

```{r pca-analysis}
#| label: Prep-data-PCA-average-ratings-by-model-and-feature

pca_data <- df_long %>%
  group_by(model, feature) %>%
  summarise(mean_rating = mean(rating, na.rm = TRUE), .groups = "drop") %>%
  pivot_wider(names_from = feature, values_from = mean_rating) %>%
  column_to_rownames("model")
```

```{r}
#| label: perform-pca
pca_result <- prcomp(pca_data, scale = TRUE, center = TRUE)

# Calculate variance explained
var_explained <- summary(pca_result)$importance[2, ] * 100
cumvar_explained <- summary(pca_result)$importance[3, ] * 100

```

```{r}
#| label: scree-plot
scree_data <- data.frame(
  PC = 1:length(var_explained),
  Variance = var_explained,
  Cumulative = cumvar_explained
)

p_scree <- ggplot(scree_data, aes(x = PC)) +
  geom_bar(
    aes(y = Variance),
    stat = "identity",
    fill = brand_colors$primary_navy
  ) +
  geom_line(aes(y = Cumulative), color = brand_colors$alert_red, size = 1.5) +
  geom_point(aes(y = Cumulative), color = brand_colors$alert_red, size = 3) +
  geom_hline(
    yintercept = 80,
    linetype = "dashed",
    color = brand_colors$accent_yellow
  ) +
  scale_x_continuous(breaks = 1:9) +
  labs(
    title = "PCA Scree Plot",
    subtitle = "Variance explained by principal components",
    x = "Principal Component",
    y = "Variance Explained (%)"
  )

print(p_scree)
```

```{r}
#| label: biplot-models-features
fviz_pca_biplot(
  pca_result,
  col.var = brand_colors$primary_navy,
  col.ind = brand_colors$accent_yellow,
  label = "all",
  labelsize = 4,
  pointsize = 3,
  repel = TRUE
) +
  labs(
    title = "PCA Biplot: Models and Features",
    subtitle = "First two principal components"
  ) +
  theme_minimal()
```

#### Interpretation of PCA Biplot

This PCA biplot summarizes the relationships between different language models and human evaluation features using the first two principal components.

The first principal component (**PC1**) explains `r round(var_explained[1], 1)`% of the total variance, while the second component (**PC2**) explains `r round(var_explained[2], 1)`%. Together, they account for `r round(cumvar_explained[2], 1)`% of the overall variance, indicating that most of the meaningful differences between models are captured in this 2D space.

**Feature Contributions**

The blue arrows represent evaluation features, where:
- The **direction** indicates the orientation of increasing values for that feature.
- The **length** reflects the strength of that feature's contribution to the variance captured by PC1 and PC2.

For example:
- Features such as **`r names(sort(abs(pca_result$rotation[, 1]), decreasing = TRUE)[1])`** and **`r names(sort(abs(pca_result$rotation[, 2]), decreasing = TRUE)[1])`** contribute most to PC1 and PC2, respectively.

**Model Comparison**

The yellow dots represent different language models. Models located near a feature arrow likely score highly on that feature. Conversely, models opposite the direction of a feature vector tend to score lower on it.

- Models like **`r rownames(pca_data)[which.max(pca_result$x[,1])]`** are positioned strongly along PC1, indicating high alignment with features like **`r names(sort(abs(pca_result$rotation[, 1]), decreasing = TRUE)[1])`**.
- In contrast, **`r rownames(pca_data)[which.min(pca_result$x[,1])]`** scores lower on those same features.

**Summary**

This plot helps visualize how models differ in their qualitative strengths. For example:
- Models positioned in the direction of **Conciseness** and **Readability** likely produce more compact and easier-to-read outputs.
- Those aligned with **Comprehensibility**, **Actionability**, and **Educational Value** may offer more informative or helpful responses, even if they are less concise.


```{r}
#| label: feature-loadings-plot-no-models

loadings_df <- as.data.frame(pca_result$rotation[, 1:2])
loadings_df$feature <- rownames(loadings_df)

p_loadings <- ggplot(loadings_df, aes(x = PC1, y = PC2, label = feature)) +
  geom_segment(
    aes(x = 0, y = 0, xend = PC1, yend = PC2),
    arrow = arrow(length = unit(0.3, "cm")),
    color = brand_colors$supporting_blue
  ) +
  geom_text_repel(size = 4, color = brand_colors$primary_navy) +
  geom_hline(yintercept = 0, linetype = "dashed", alpha = 0.5) +
  geom_vline(xintercept = 0, linetype = "dashed", alpha = 0.5) +
  labs(
    title = "Feature Loadings on First Two Principal Components",
    x = sprintf("PC1 (%.1f%% variance)", var_explained[1]),
    y = sprintf("PC2 (%.1f%% variance)", var_explained[2])
  ) +
  coord_equal()

print(p_loadings)
```

The above plot simply shows how each feature contributes to PCs.

This feature loadings plot specifically emphasizes:
- Which features contribute most to PC1 and PC2.
- How features relate to each other (e.g., clusters or oppositions).
- Avoids clutter from model points — it's cleaner for purely feature-based interpretation.

**Interpretation:**

- Conciseness and Readability are the strongest contributors, pulling in opposite directions along PC1 and PC2 respectively.
- Most other features (e.g., Actionability, Completeness, Educational Value) cluster closely together with small loadings, contributing moderately and similarly to PC1.

**The Core Trade-off**

The analysis reveals a fundamental tension in AI model design: **Comprehensiveness vs. Conciseness** - PC1 (88.8% of variance) essentially captures this trade-off. Models can either be thorough, helpful, and comprehensive OR concise and to-the-point, but excelling at both simultaneously appears challenging.

This makes intuitive sense - providing complete, educational, empathetic responses often requires more words and detail, which inherently conflicts with being concise.


```{r}
#| label: loadings-table

loadings_table <- loadings_df %>%
  arrange(desc(abs(PC1))) %>%
  mutate(
    PC1 = sprintf("%.3f", PC1),
    PC2 = sprintf("%.3f", PC2)
  ) %>%
  select(Feature = feature, PC1, PC2)

kable(
  loadings_table,
  caption = "Table 6. Feature loadings on first two principal components",
  # format = "latex",
  booktabs = TRUE
) %>%
  kable_styling(latex_options = c("striped", "hold_position"))
```

### 5.2 Cluster Analysis of Models

```{r cluster-analysis}
#| label: cluster-setup

dist_matrix <- dist(pca_data, method = "euclidean")
hclust_result <- hclust(dist_matrix, method = "ward.D2")
```

```{r}
#| label: dendrogram-plot1

dend <- as.dendrogram(hclust_result)
dend <- color_branches(
  dend,
  k = 3,
  col = c(
    brand_colors$primary_navy,
    brand_colors$accent_yellow,
    brand_colors$supporting_blue
  )
)

# Create dendrogram plot
p_dendrogram <- ggplot(as.ggdend(dend)) +
  labs(
    title = "Hierarchical Clustering of Models",
    subtitle = "Based on average ratings across all features",
    x = "Model",
    y = "Distance"
  ) +
  theme(axis.text.x = element_text(angle = 0, hjust = 0.5))

print(p_dendrogram)
```

```{r}
#| label: dendrogram-plot-fviz

fviz_dend(
  hclust_result,
  k = 3,
  cex = 0.6, # size of labels
  k_colors = c(
    brand_colors$primary_navy,
    brand_colors$accent_yellow,
    brand_colors$supporting_blue
  ),
  main = "Hierarchical Clustering of Models",
  sub = "Based on average ratings across all features",
  xlab = ""
)
```

```{r}
#| label: K-means-clustering

set.seed(123)
kmeans_result <- kmeans(pca_data, centers = 3, nstart = 25)
```

```{r}
#| label: kmeans-clusters-plot

pca_scores <- as.data.frame(pca_result$x[, 1:2])
pca_scores$model <- rownames(pca_scores)
pca_scores$cluster <- factor(kmeans_result$cluster)

p_clusters <- ggplot(
  pca_scores,
  aes(x = PC1, y = PC2, color = cluster, label = model)
) +
  geom_point(size = 4) +
  geom_text_repel(size = 4) +
  scale_color_manual(
    values = c(
      brand_colors$primary_navy,
      brand_colors$accent_yellow,
      brand_colors$supporting_blue
    )
  ) +
  labs(
    title = "Model Clusters in PCA Space",
    subtitle = "K-means clustering (k=3)",
    x = sprintf("PC1 (%.1f%% variance)", var_explained[1]),
    y = sprintf("PC2 (%.1f%% variance)", var_explained[2]),
    color = "Cluster"
  )

print(p_clusters)
```

### 5.3 Auto-grader vs Human Consensus Analysis

```{r auto-human-comparison}
#| label: human-consensus-scores

human_consensus <- df_long %>%
  filter(grader_type == "human") %>%
  group_by(index, model, feature) %>%
  summarise(human_mean = mean(rating, na.rm = TRUE), .groups = "drop")

# Calculate individual auto-grader correlations with human consensus
auto_correlations <- df_long %>%
  filter(grader_type == "auto") %>%
  left_join(human_consensus, by = c("index", "model", "feature")) %>%
  group_by(rater_name, feature) %>%
  summarise(
    correlation = cor(rating, human_mean, use = "complete.obs"),
    n_pairs = sum(!is.na(rating) & !is.na(human_mean)),
    .groups = "drop"
  )
```

```{r}
#| label: auto-human-correlation-heatmap
correlation_matrix <- auto_correlations %>%
  pivot_wider(names_from = feature, values_from = correlation) %>%
  column_to_rownames("rater_name") %>%
  select(-n_pairs) %>%
  as.matrix()

# Create heatmap
p_auto_human_corr <- correlation_matrix %>%
  as.data.frame() %>%
  rownames_to_column("Auto_Grader") %>%
  pivot_longer(
    -Auto_Grader,
    names_to = "Feature",
    values_to = "Correlation"
  ) %>%
  mutate(
    Auto_Grader = case_when(
      str_detect(Auto_Grader, "openai_4o_strict") ~ "OpenAI GPT-4o (Stricter)",
      str_detect(Auto_Grader, "openai_4o") ~ "OpenAI GPT-4o",
      str_detect(Auto_Grader, "gemini_2.5_pro") ~ "Gemini 2.5 Pro",
      str_detect(Auto_Grader, "gemini_2.0_flash") ~ "Gemini 2.0 Flash",
      str_detect(Auto_Grader, "anthropic_somnet4") ~ "Anthropic Sonnet 4",
      str_detect(Auto_Grader, "anthropic_somnet3.5") ~ "Anthropic Sonnet 3.5",
      TRUE ~ Auto_Grader
    ),
    Feature = factor(Feature, levels = rating_features)
  ) %>%
  ggplot(aes(x = Feature, y = Auto_Grader, fill = Correlation)) +
  geom_tile(color = "white", size = 0.5) +
  geom_text(aes(label = sprintf("%.2f", Correlation)), size = 3) +
  scale_fill_gradient2(
    low = brand_colors$alert_red,
    mid = "white",
    high = brand_colors$success_green,
    midpoint = 0.7,
    limits = c(0.4, 1),
    name = "Correlation"
  ) +
  labs(
    title = "Auto-grader Agreement with Human Consensus",
    subtitle = "Pearson correlation coefficients",
    x = "Feature",
    y = "Auto-grader"
  ) +
  theme(
    axis.text.x = element_text(angle = 45, hjust = 1),
    axis.title.x = element_blank(),
    axis.title.y = element_blank(),
    legend.position = "right",
  )

print(p_auto_human_corr)
```

```{r}
#| label: auto-grader-performance-metrics-vs-human-consensus

auto_performance <- df_long %>%
  filter(grader_type == "auto") %>%
  left_join(human_consensus, by = c("index", "model", "feature")) %>%
  group_by(rater_name) %>%
  summarise(
    RMSE = sqrt(mean((rating - human_mean)^2, na.rm = TRUE)),
    MAE = mean(abs(rating - human_mean), na.rm = TRUE),
    Bias = mean(rating - human_mean, na.rm = TRUE),
    Correlation = cor(rating, human_mean, use = "complete.obs"),
    .groups = "drop"
  ) %>%
  mutate(rater_name = str_remove(rater_name, "autoLLM_"))

# Create performance comparison plot
p_auto_performance <- auto_performance %>%
  mutate(
    rater_name = case_when(
      str_detect(rater_name, "openai_4o_strict") ~ "OpenAI GPT-4o (Stricter)",
      str_detect(rater_name, "openai_4o") ~ "OpenAI GPT-4o",
      str_detect(rater_name, "gemini_2.5_pro") ~ "Gemini 2.5 Pro",
      str_detect(rater_name, "gemini_2.0_flash") ~ "Gemini 2.0 Flash",
      str_detect(rater_name, "anthropic_somnet4") ~ "Anthropic Sonnet 4",
      str_detect(rater_name, "anthropic_somnet3.5") ~ "Anthropic Sonnet 3.5",
      TRUE ~ rater_name
    )
  ) %>%
  pivot_longer(
    cols = c(RMSE, MAE, Bias, Correlation),
    names_to = "Metric",
    values_to = "Value"
  ) %>%
  ggplot(aes(x = rater_name, y = Value, fill = Metric)) +
  geom_bar(stat = "identity", position = "dodge") +
  facet_wrap(~Metric, scales = "free_y", ncol = 2) +
  scale_fill_manual(
    values = c(
      "RMSE" = brand_colors$alert_red,
      "MAE" = brand_colors$accent_yellow,
      "Bias" = brand_colors$supporting_blue,
      "Correlation" = brand_colors$success_green
    )
  ) +
  labs(
    title = "Auto-grader Performance Metrics",
    subtitle = "Comparison with human consensus ratings",
    x = "Auto-grader",
    y = "Value"
  ) +
  theme(
    axis.text.x = element_text(angle = 45, hjust = 1),
    legend.position = "none"
  )

print(p_auto_performance)

```

```{r}
#| label: auto-grader-performance-table

auto_performance_table <- auto_performance %>%
  mutate(
    RMSE = sprintf("%.3f", RMSE),
    MAE = sprintf("%.3f", MAE),
    Bias = sprintf("%.3f", Bias),
    Correlation = sprintf("%.3f", Correlation)
  ) %>%
  rename(`Auto-grader` = rater_name)

auto_performance_table %>%
  mutate(
    `Auto-grader` = case_when(
      str_detect(`Auto-grader`, "openai_4o_strict") ~
        "OpenAI GPT-4o (Stricter)",
      str_detect(`Auto-grader`, "openai_4o") ~ "OpenAI GPT-4o",
      str_detect(`Auto-grader`, "gemini_2.5_pro") ~ "Gemini 2.5 Pro",
      str_detect(`Auto-grader`, "gemini_2.0_flash") ~ "Gemini 2.0 Flash",
      str_detect(`Auto-grader`, "anthropic_somnet4") ~ "Anthropic Sonnet 4",
      str_detect(`Auto-grader`, "anthropic_somnet3.5") ~ "Anthropic Sonnet 3.5",
      TRUE ~ `Auto-grader`
    )
  ) %>%
  gt() %>%
  cols_label(
    RMSE = "Root Mean Square Error",
    MAE = "Mean Absolute Error",
    Bias = "Bias",
    Correlation = "Correlation"
  ) %>%
  fmt_number(
    columns = c(RMSE, MAE, Bias, Correlation),
    decimals = 3
  ) %>%
  tab_header(
    title = "Table 7. Auto-grader performance metrics compared to human consensus"
  ) %>%
  opt_row_striping()
```

---

## 6. Summary and Key Findings

### 6.1 Overall Performance Summary

```{r summary-visualization}
#| label: panel-a-model-rankings

panel_a <- df_long %>%
  mutate(
    model = factor(tools::toTitleCase(as.character(model)))
  ) %>%
  group_by(model) %>%
  summarise(
    mean_rating = mean(rating, na.rm = TRUE),
    se = sd(rating, na.rm = TRUE) / sqrt(n()),
    .groups = "drop"
  ) %>%
  ggplot(aes(x = reorder(model, mean_rating), y = mean_rating)) +
  geom_bar(stat = "identity", fill = brand_colors$primary_navy) +
  geom_errorbar(
    aes(ymin = mean_rating - se, ymax = mean_rating + se),
    width = 0.3
  ) +
  coord_flip() +
  labs(
    title = "A. Overall Model Performance",
    x = "Model",
    y = "Mean Rating"
  ) +
  theme_minimal(base_size = 16)
```

```{r}
#| label: panel-b-rater-type-agreement

panel_b <- df_long %>%
  group_by(rater_type_detailed) %>%
  summarise(
    mean_rating = mean(rating, na.rm = TRUE),
    sd = sd(rating, na.rm = TRUE),
    .groups = "drop"
  ) %>%
  ggplot(aes(
    x = reorder(rater_type_detailed, -mean_rating),
    y = mean_rating,
    fill = rater_type_detailed
  )) +
  geom_bar(stat = "identity") +
  geom_errorbar(
    aes(ymin = mean_rating - sd, ymax = mean_rating + sd),
    width = 0.3
  ) +
  scale_fill_manual(
    values = c(
      "Student" = brand_colors$supporting_blue,
      "Expert" = brand_colors$primary_navy,
      "Auto-grader" = brand_colors$accent_yellow
    )
  ) +
  labs(
    title = "B. Rater Type Comparison",
    x = "Rater Type",
    y = "Mean Rating"
  ) +
  theme(
    legend.position = "none",
    axis.title.x = element_blank()
  )

ggsave(
  "figures/panel_b_rater_type_agreement.pdf",
  panel_b,
  width = 4,
  height = 4
)

```

```{r}
#| label: panel-c-auto-grader-accuracy

panel_c <- auto_performance %>%
  mutate(
    rater_name = case_when(
      str_detect(rater_name, "openai_4o_strict") ~ "OpenAI GPT-4o (Stricter)",
      str_detect(rater_name, "openai_4o") ~ "OpenAI GPT-4o",
      str_detect(rater_name, "gemini_2.5_pro") ~ "Gemini 2.5 Pro",
      str_detect(rater_name, "gemini_2.0_flash") ~ "Gemini 2.0 Flash",
      str_detect(rater_name, "anthropic_somnet4") ~ "Anthropic Sonnet 4",
      str_detect(rater_name, "anthropic_somnet3.5") ~ "Anthropic Sonnet 3.5",
      TRUE ~ rater_name
    )
  ) %>%
  ggplot(aes(x = reorder(rater_name, -Correlation), y = Correlation)) +
  geom_bar(stat = "identity", fill = brand_colors$success_green) +
  geom_hline(
    yintercept = 0.8,
    linetype = "dashed",
    color = brand_colors$alert_red
  ) +
  labs(
    title = "C. Auto-grader Correlation with Humans",
    x = "Auto-grader",
    y = "Correlation"
  ) +
  theme_minimal(base_size = 16) +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))
```

```{r}
# Combine panels
summary_plot <- (panel_a) /
  (panel_b | panel_c) +
  plot_annotation(
    title = "Figure 8. Summary of Key Findings",
    subtitle = "Comprehensive overview of model performance, feature reliability, and rater agreement"
  )

print(summary_plot)

# Save for publication
ggsave(
  "figures/figure8_summary_findings.pdf",
  summary_plot,
  width = 12,
  height = 10
)
```

### 6.2 Key Statistical Findings

```{r key-findings-table}
#| label: key-findings-summary

key_findings <- data.frame(
  Metric = c(
    "Best Performing Model",
    "Most Reliable Feature",
    "Least Reliable Feature",
    "Human-Auto Agreement",
    "Features with Significant Model Differences"
  ),
  Finding = c(
    model_performance %>%
      arrange(desc(mean_rating)) %>%
      slice(1) %>%
      pull(model) %>%
      as.character(),
    feature_performance %>%
      arrange(desc(mean_rating)) %>%
      slice(1) %>%
      pull(feature) %>%
      as.character(),
    feature_performance %>%
      arrange(mean_rating) %>%
      slice(1) %>%
      pull(feature) %>%
      as.character(),
    sprintf("r = %.3f", mean(auto_performance$Correlation)),
    paste(significant_features, collapse = ", ")
  ),
  Details = c(
    sprintf(
      "Mean rating = %.2f",
      model_performance %>%
        arrange(desc(mean_rating)) %>%
        slice(1) %>%
        pull(mean_rating)
    ),
    sprintf(
      "Mean rating = %.2f",
      feature_performance %>%
        arrange(desc(mean_rating)) %>%
        slice(1) %>%
        pull(mean_rating)
    ),
    sprintf(
      "Mean rating = %.2f",
      feature_performance %>%
        arrange(mean_rating) %>%
        slice(1) %>%
        pull(mean_rating)
    ),
    "Average across all auto-graders",
    paste(significant_features, collapse = ", ")
  )
)
```

```{r}
#| label: gt-key-findings-table

key_findings %>%
  gt() %>%
  cols_label(
    Metric = "Metric",
    Finding = "Finding",
    Details = "Details"
  ) %>%
  tab_header(
    title = "Table 8. Summary of Key Statistical Findings"
  ) %>%
  tab_options(
    table.width = pct(100),
    data_row.padding = px(2)
  ) %>%
  opt_row_striping()
```

## 7. Supplementary Materials

### 7.1 Model Diagnostics

```{r model-diagnostics}
#| label: model-diagnostics-lmm-samplefeature-appropriateness

representative_model <- lmm_results[["Appropriateness"]]$model

# Create diagnostic plots
p_diagnostics <- plot(
  check_model(representative_model),
  colors = c(brand_colors$primary_navy, brand_colors$alert_red)
)
```

```{r}
#| label: qq-plot-random-effects

# plotting the normality of the random intercepts for index (questions)

ranef_df <- ranef(representative_model)$index %>% as.data.frame()

p_qq_random <- ggplot(ranef_df, aes(sample = `(Intercept)`)) +
  stat_qq() +
  stat_qq_line(color = brand_colors$alert_red) +
  labs(
    title = "QQ Plot of Random Effects (Query ID)",
    x = "Theoretical Quantiles",
    y = "Sample Quantiles"
  )

print(p_qq_random)
```

```{r}
#| label: qq-plot-random-effects-raters

ranef_raters <- ranef(representative_model)$rater_name %>% as.data.frame()

ggplot(ranef_raters, aes(sample = `(Intercept)`)) +
  stat_qq() +
  stat_qq_line(color = brand_colors$alert_red) +
  labs(
    title = "QQ Plot of Random Effects (Rater)",
    x = "Theoretical Quantiles",
    y = "Sample Quantiles"
  )
```


## References

```{r save-session-info}
# Save session information for reproducibility
sink("./notebooks/stats_analysis_session/session_info.txt")
sessionInfo()
sink()

# Print completion message
cat(
  "\nAnalysis completed successfully. All figures saved to 'figures/' directory.\n"
)
```